[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "Preface\nHere are resources that are being used for the Advanced Statstics course taught to Economics Students at The University of Manchester\nRalf Becker",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 An introductory example\nLet’s start this with a problem. A problem which clearly illustrates why we need statistics. Put your mind back to the beginning of the Covid-19 pandemic. Very quickly after understanding the health risks by the Covid-19 virus in early 2020 attention turned to the one health tool that was very quickly identified as the “game changer” in the fight against the virus. A vaccine. In the time series plot below (Figure 2.1) you can see how often people worldwide searched for the term “Covid-19 vaccine”.\nVery quickly many different research teams, in private companies, universities and government institutions started researching such vaccines. Very quickly the first companies published results of trials to investigate whether the vaccines they had developed worked. Here is a link to one of the first publication of preliminary results by the Pfizer/Biontech consortium from 18 November 2020.\nIn this news story you can find the following information\nThe information on the efficacy was pounced on by news organisations, in particular the 95% efficacy rate. Let us look at how this is calculated. There were 170 cases of covid amongst the around 40,000 trial participants. Around half the trial participants had received the Biontech vaccince (the treatment group) whereas the other half had received a placebo (meaning they also had a shot, but it did not contain any vaccine, the Control group). The numbers in the two groups were about equal (a piece of information not immediately obvious from the news story but available from other sources.)\nThe way how the efficacy measure is calculated is as follows\n\\[\\begin{equation*}\n    efficacy = 1 -\\frac{\\text{Number of cases in Treatment Group}}{\\text{Number of cases in Control Group}}=1-\\frac{8}{162}=0.9506=95.06\\%\n\\end{equation*}\\]\nThat is the 95% figure quoted in the above news story. It gives you the reduction in probability of getting the disease by being vaccinated.\nYou can see that from a different perspective, using additional data from David Spiegelhalter’s excellent little book Covid by Numbers.\nIn the last line of the above Table you can see that the probability of contracting the Covid-19 virus in the Control Group was 0.884%. One way to think about efficacy is to think about how much this risk is reduced by having received a vaccine. An efficacy of 95% suggests that this risk is reduced by 95%, hence the risk should be 0.884%∗5%=0.884%∗0.05=0.0442% which is indeed approximately the number you see in the above table.\nHere is the BBC reporting on these trial results on 9 November 2020.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#an-introductory-example",
    "href": "intro.html#an-introductory-example",
    "title": "1  Introduction",
    "section": "",
    "text": "Figure 1.1: Search popularity for “Covid-19 vaccine\n\n\n\n\n\n\n“Analysis of the data indicates a vaccine efficacy rate of 95% (p&lt;0.0001) in participants without prior SARS-CoV-2 infection […].”\n“The first primary objective analysis is based on 170 cases of COVID-19, as specified in the study protocol, of which 162 cases of COVID-19 were observed in the placebo group versus 8 cases in the BNT162b2 group.”\n“The Phase 3 clinical trial of BNT162b2 began on July 27 and has enrolled 43,661 participants to date, 41,135 of whom have received a second dose of the vaccine candidate as of November 13, 2020.”\n“Efficacy was consistent across age, gender, race and ethnicity demographics. The observed efficacy in adults over 65 years of age was over 94%.”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTreatment Group (vaccine)\nControl Group (placebo)\n\n\n\n\nPeople in group\n18,198\n18,325\n\n\nNumber of Covid Cases\n8\n162\n\n\nProportion getting the disease\n0.043%\n0.884%\n\n\n\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nConsider the data in the following table. Calculate each vaccine’s efficacy to 4 decimal places (which will be the standard precision used here). Identify which vaccine has the highest efficacy. You can assume that all results come from a randomised control trial with equal sized Treatment and Control Groups. If you calculated a 76.43% efficacy your answer should be 0.7643.\n\n\n\n\n\n\n\n\n\nVaccine Name\nInfections in treatment Group\nInfections in Control Group\nEfficacy (to 4dp)\n\n\n\n\nHeal all\n5\n134\n\n\n\nBe careful\n11\n109\n\n\n\nElderVax\n11\n138\n\n\n\nInlightVax\n9\n137\n\n\n\n\n\n\nFeedback\n\nWe do not need the number of people in the survey as we assume that the numbers are the same in both groups. Therefore we can calculate the efficacy (e.g. for “Heal all”) as \\(1 - (5/134) = 0.9627\\). If you had different sample sizes in the experimental (\\(n_e\\)) and control group (\\(n_c\\)) you would calculate \\(1-((5/n_e)/(134/n_c))\\). From here you can see that \\(n_c\\) and \\(n_e\\) would cancel out if they were the same.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#and-where-is-the-statistics-in-this",
    "href": "intro.html#and-where-is-the-statistics-in-this",
    "title": "1  Introduction",
    "section": "1.2 And where is the statistics in this?",
    "text": "1.2 And where is the statistics in this?\nAt the completion of the course you will realise that behind these numbers there are a number of interesting questions one may want to ask.\n\nAs we have a sample of data, can we express how certain we are about the 95% efficacy?\nThe WHO demands that vaccines should have a 50% efficacy, given our uncertainty, do we believe that this vaccine if effective?\nSurely the efficacy may vary with some demographic factors (e.g. age, gender, ethnicity). Can we figure out which demographic factors make the vaccine more effective?\n\nIt turns out that statisticians can help you will these type of questions and more!\nThe subject of statistics is concerned with scientific methods for collecting, organizing, summarizing and presenting data (numerical information). The power and utility of statistics derives from being able to draw valid conclusions (inferences), and make reasonable decisions, on the basis the available data. (The term statistics is also used in a much narrower sense when referring to the data themselves or various other numbers derived from any given set of data. Thus we hear of employment statistics (% of people unemployed), accident statistics, (number of road accidents involving drunk drivers), etc.)\nData arise in many spheres of human and natural activity. Such data may be obtained as a matter of course (e.g., meteorological records, daily closing prices of shares, monthly interest rates, etc.), or they may be collected by survey or experiment for the purposes of a specific statistical investigation. An example of an investigation using statistics was the Survey of British Births, 1970, the aim of which was to improve the survival rate and care of British babies at or soon after birth. To this end, data on new born babies and their mothers were collected and analysed. The Family Expenditure Survey regularly collects information on household expenditure patterns - including amounts spent on lottery tickets.\nOften different methods are used to effectively collect information on the same issue. Especially when the issue is very important it is often useful to have several different sources of data in order to cross-check the findings. Take, as an example, the issue of measuring how many people have been infected by Covid-19. In the UK, several sources have been used to estimate these numbers. The most attention was given to the number of positive test results reported. On the other hand, the Office for national Statistics also ran a survey across the entire population (hoping to generate a sample as representative as possible), the COVID-19 Infection Survey.\n\n\n\n\n\n\nImportantExercise\n\n\n\nWhich of the following is not an actual surveys run in the UK? Of course you are not expected to know this by heart, but you should be able quickly confirm whether these exist via a websearch.\n\n Crime Survey for England and Wales Active Lifestyle Survey, Measuring sporting activities in UK Pensioners General Lifestyle Survey National Travel Survey Young People’s Social Attitudes Survey\n\n\n\n\nFeedback\n\nAll but the “Active Lifestyle Survey, Measuring sporting activities in UK Pensioners” surveys are real surveys. A good overview of many UK surveys is available from the UK Data Service. While the above survey does not exist, there is in fact a survey investigating the amount of activities done by people in the UK. This survey is organised by Sport England but does not specifically focus on pensioners.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-we-will-learn",
    "href": "intro.html#what-we-will-learn",
    "title": "1  Introduction",
    "section": "1.3 What we will learn",
    "text": "1.3 What we will learn\nYou can divide the material you will learn in this course in three sections.\n\nDescriptive Statistics\nProbability Calculus\nInferential Statistics\n\nLet me give you short descriptions of what these sections will be about and how they will fit together. As you go through the material I recommend that you return to this short section such that you never loose sight of the bigger picture.\n\n1.3.1 Descriptive Statistics\nIn this section we will look at a sample of data and use a variety of methods to describe what we see in the sample. Say you have just received the data for the 36,514 subjects in the Biontech Covid vaccine trial. Your supervisor asks you to summarise the data for her. Summarising means saying something about the data with less than the 36,514 pieces of information.\nWe will find ways to summarise the information with a few numbers (statistics). For instance the proportion of subjects who got infected with covid. You may also want to present some graphical ways to summarise the data. This type of activity has value in itself. In fact a lot of the data presented in the news is basically done with the use of summary statistics.\n\n\n1.3.2 Probability Calculus\nIf you play any games like roulette, cards (e.g. poker or blackjack or even UNO) or any other games of chance you will be aware that outcomes of many events are random. Importantly, random doesn’t mean that we don’t know anything about the potential outcomes. Take a dice. Before you roll the dice you do not know the outcome, but what you do know is that the probability of each outcome is 1/6. In other words we know the probability distribution of the random variable.\nIn the probability section you will learn to calculate with the properties of random variables. In particular you will learn to calculate expected values, variances and correlations between multiple random variables. You will also learn about a number of common, even famous, random distributions.\n\n\n1.3.3 Inferential Statistics\nIn some sense this is the high point of the statistical knowledge you will learn in this unit. We will often have information about a particular sample (like the sample from the Biontech vaccine trial), hopefully drawn at random. Using descriptive statistics we can find out everything there is to know about the sample (e.g. mean, variance, skewness, correlation if we have a sample on multiple variables). However, usually we are not really interested in the sample itself, the sample is merely a window into the population and we really want to know things about the population (see the schematic in (fig_inference?)). In terms of the example, we are interested in how the vaccine would work in the population.\n\n\n\nSchematic of Statistical Inference and how it relates to descriptive statistics and probability calculus.\n\n\nIn order to learn anything from the sample about the population we need two key tools. First, we need to be able to describe the sample (descriptive statistics) and we need to learn how to calculate with probabilities. After all we will not be able to make statements about the population with certainty, we will only be able to make probabilistic statements. Here is a video discussion of the basics of statistical inference (7.44 min).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "datatypes.html",
    "href": "datatypes.html",
    "title": "2  Data Types",
    "section": "",
    "text": "2.1 Types of data\nBroadly speaking, by data we mean numerical values associated with some variable of interest. Different types of data may need special treatment when it comes to statistical analysis. For this reason, it is important to be able to distinguish a few key features. A variable can produce data that are either of continuous or discrete nature (see below for examples).\nAnother level at which variables differ is whether they are sampled in time or in a cross-section.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "datatypes.html#types-of-data",
    "href": "datatypes.html#types-of-data",
    "title": "2  Data Types",
    "section": "",
    "text": "2.1.1 Discrete data\nThe variable \\(X\\) is said to be discrete if it can only ever yield isolated values some of which (if not all) are often repeated in the sample. It is, however, important to note that there are different types of discrete data:\n\nORDINAL. Here the categories have a natural ordering.\nExamples: Football Leagues: Premier League, Championship, etc.\nNOMINAL. Here there is no natural ordering to the categories.\nExamples: Gender: Male, Female\nCOUNT. A variable that represents the counts of certain events.\nExamples: Number of children in household: 0,1,2,3,etc.\n\n\n\n\n\n\n\nWarningData collection is not always neutral\n\n\n\nBefore moving on from here it is important to understand that the mere process of collecting data may not be an innocent or totally neutral process. For instance the above note on gender. Traditionally you would indeed have found questions like “What is your gender?” with “Male” and “Female” being the only options. It is now widely recognised that for some people the gender they identify as is not the same as the sex they were born in. As a consequence of this you will now find that sometimes questions are split, one asking for the sex and the other for gender identification. Both public and private institutions think carefully about how to ask such questions as demonstrated by the guidance of the Office for National Statistics (ONS) and by Survey Monkey.\n\n\n\n\n2.1.2 Continuous data\nThe variable \\(Y\\) is said to be continuous if it can assume any value taken (more or less) from a continuum (a continuum is an interval, or range of numbers). A nice way to distinguish between a discrete and continuous variable is to consider the possibility of listing possible values. It is theoretically impossible to even begin listing all possible values that a continuous variable \\(Y\\) could assume. However, this is not so with a discrete variable; you may not always be able to finish the list, but at least you can make a start.\nFor example, the birth-weight of babies is an example of a continuous variable. There is no reason why a baby should not have a birth weight of \\(2500.0234\\) grams, even though it wouldn’t be measured as such! Try to list all possible weights (in theory) bearing in mind that for any two weights that you write down, there will always be another possibility half way between these. We see, then, that for a continuous variable an observation is recorded, as the result of applying some measurement, but that this inevitably gives rise to a rounding (up or down) of the actual value. (No such rounding occurs when recording observations on a discrete variable.)\nA variable can be continuous even though it is defined on a limited scale. For instance the weight variable has a limited scale as weights cannot be negative.\nFinally, note that for a continuous variable, it is unlikely that values will be repeated frequently in the sample, unless rounding occurs.\nOther examples of continuous data include: heights of people; volume of water in a reservoir; and, to a workable approximation, Government Expenditure. One could argue that the last of these is discrete (due to the finite divisibility of monetary units). However, when the amounts involved are of the order of millions of pounds, changes at the level of individual pence are hardly discernible and so it is sensible to treat the variable as continuous.\n\n\n2.1.3 Additional resources\n\nKhan Academy: Discusses example of continuous and discrete random variables.\n\n\n\n\n\n\n\nImportantExercises\n\n\n\nConsider if the following set of data is discrete or continuous:\nThe heights of your friends\nDiscreteContinuous\nThe number of books on your bookcase\nDiscreteContinuous\n\n\n\n\n2.1.4 Cross-section data\nCross-section data comprises observations on a particular variable taken at a single point in time. For example: annual crime figures recorded by Police regions for the year 1999; the birth-weight of babies born, in a particular maternity unit, during the month of April 1998; initial salaries of graduates from the University of Manchester, 2012.\nNote, the defining feature is that there is no natural ordering in the data you have. Say you have birth weights of babies Sarah, Lin, Max and Nikolai, then it doesn’t matter if you present that data in that order or if you re-order the data in any way, e.g. Nikola, Sarah, Max and Lin. This will be different for time-series data.\n\n2.1.4.1 Example\nExchange Rates: From Oanda.com\nHere you can see a lot of different exchange rates, all recorded at the same time. Note, however, that they are updated every 5 minutes. So you can see that whether you are dealing with cross-section or time-series data (see below) depends on your perspective.\n\n\n\n2.1.5 Time-series data\nOn the other hand, time-series data are observations on a particular variable recorded over a period of time, at regular intervals. For example personal crime figures for Greater Manchester recorded annually from 1980-2012; monthly household expenditure on food; the daily closing price of a certain stock. In this case, the data does have a natural ordering since they are measured from one time period to the next.\n\n2.1.5.1 Example\nExchange Rates: From USD v EURO exchange rate from the ECB\nAn example for a time-series of an exchange rate, here USD/EUR.\n\n\n\n\n\n\nFigure 2.1: USD/Euro exchange rate\n\n\n\n\n\n\n\n\n\nImportantExercises\n\n\n\nConsider if the following set of data is discrete or continuous:\nStudy hours data from students responding to a survey after the 2nd week of semester.\nTime-Series DataCross-Section Data\nTemperatures at 8am on the 22 Jan 2026 from all British cities\nTime-Series DataCross-Section Data\nTemperatures at 8am in Manchester for all days in 2025\nTime-Series DataCross-Section Data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "datatypes.html#population-and-sample-data",
    "href": "datatypes.html#population-and-sample-data",
    "title": "2  Data Types",
    "section": "2.2 Population and Sample Data",
    "text": "2.2 Population and Sample Data\nWhen dealing with data we will have to be aware of what the difference between population and sample data are. The population represents all the data you are interested in. Imagine you have asked all households in your postcode area for their annual income. In case you are interested in the incomes in your postcode only, then your observations represent the population data. If however, you are really interested in the average household income of ALL households in Manchester, then the data you have collected in your postcode are a sample only. As you can see, the same set of observations can be either a sample or population, it really depends on the question you are asking.\n\n\n\n\n\n\nImportantRepresentative Sampling\n\n\n\nBriefly thinking about the example above where you collect all data from one postcode to be a sample for all of Manchester, it quickly becomes obvious that this would not be a good sampling strategy. If you were fortunate enough to live in a posh are (e.g. Hale Barns) it would be very obvious that the incomes you collected would not be very representative of the incomes across the whole of Manchester.\nIf you want a representative sample you will have to be very careful in the way you design your sampling strategy to achieve that. This is super important. Here we will not cover any more details on this and when we use survey data we will assume that the data are representative. If you are interested in this topic a good resource is: Czaja, R. and Blair, J. (2013) Designing Surveys: a guide to decisions and procedures. 3rd ed. Sage, London.\n\n\nGiven any particular question, we always prefer to have population data, however in most cases one will have to accept that one can only get sample data. Consider, for instance, a political analyst who is interested in the voting intentions of all eligible voters in the UK. As there are many millions voters it is virtually impossible to obtain population data. You will all be familiar with opinion polls who will be based on the basis of a sample of eligible voters (usually somewhere around 1000 voters).\nThe interesting question is then the following: Given the analyst is really interested in the voting intention of ALL voters, how can the sample information be used to learn something about the population? This problem is at heart of the problem of statistical inference.\n\n\n\n\n\n\nImportantExercises\n\n\n\nConsider if the following set of data is discrete or continuous:\nA safety inspector conducts air quality tests on a randomly selected group of 16 classrooms at a primary school.\n\n The population is all classrooms in the primary school; the sample is the 16 classrooms The population is all primary students in the school; the sample is the students in the 16 classrooms selected. The population is all classrooms in the district; the sample is the 16 classrooms selected.\n\n\n\nFeedback\n\nThe population is the entire group of people or things we want to study. The sample is the part of the population that we actually collect data from.\nThe safety inspector tested classrooms in the primary school, not students. Further, the safety inspector only tested classrooms in one school, not in all the schools in the district. So the safety inspector selected 16 classrooms.\nThe population is all classrooms in the primary school; the sample is the 16 classrooms selected.\n\nThe Director of A High School surveyed a random sample of 200 of their juniors to see how juniors at the school felt about the lunch offering at the school’s cafeteria.\n\n The population is all high school juniors in the world; the sample is all of the juniors at A High. The population is all students at A High; the sample is all of the juniors at A High The population is all juniors at A High School; the sample is the 200 juniors surveyed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "datatypes.html#summary-data-types",
    "href": "datatypes.html#summary-data-types",
    "title": "2  Data Types",
    "section": "2.3 Summary Data Types",
    "text": "2.3 Summary Data Types\nIn this lesson you learned that there are different data types, discrete and continuous, cross-section and time-series data. You would learn later, that different data types require the application of different tools. These may be different descriptive tools but also different probability tools.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "graphical_descriptive.html",
    "href": "graphical_descriptive.html",
    "title": "3  Graphical Descriptive Statistics",
    "section": "",
    "text": "3.1 Introduction\nYou will often be confronted with a huge amount of data, say around 50,000 road traffic accidents in Greater Manchester in 2022. Your boss (perhaps the Major of Manchester) wants to have an overview of the most important features of these accidents. It is obvious that you cannot just give him (him, as at the time of writing the Major of Manchester is Andy Burnham) the spreadsheet of the data. You need to summarise! You need to prepare some representations of the data that give a good overview of what is going on. That is what descriptive statistics do.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "graphical_descriptive.html#graphical-data-representations",
    "href": "graphical_descriptive.html#graphical-data-representations",
    "title": "3  Graphical Descriptive Statistics",
    "section": "3.2 Graphical Data Representations",
    "text": "3.2 Graphical Data Representations\nWe shall now describe some simple graphical displays, which provide visual summaries of the raw data. We consider just 4 types: those which can be used with discrete data - the relative frequency diagram, or bar chart; those for use with continuous data - the histogram; those that can be used to represent time-series data line graphs; and those which provide a summary of the possible relationship between two variables - the scatter plot. Each are introduced by means of a simple example.\nOur introduction here barely scratches the surface and the field of data visualisation has recently become more prominent. If you want to explore a huge variety of data visualisations (very few of which could be created with Excel) the following are a few useful links:\n\nVisualisingdata,\nVisme or,\nthese examples discussed on the Tableau webpage including some truly iconic visualisations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "graphical_descriptive.html#discrete-data---bar-charts",
    "href": "graphical_descriptive.html#discrete-data---bar-charts",
    "title": "3  Graphical Descriptive Statistics",
    "section": "3.3 Discrete data - Bar Charts",
    "text": "3.3 Discrete data - Bar Charts\nWe will use the Titanic Data set which contains three discrete data sets, the class in which the passenger resided (1st, 2nd or 3rd), the gender and whether a passenger survived the sinking of the Titanic on 15 April 1912.\nLet’s concentrate on the information of the class in which the Titanic passengers were booked into. This is a snippet of the data in the spreadsheet:\n\n\n\n\n\n\nFigure 3.1: Data of Titanic passengers\n\n\n\nBy summarising the data we find the following summary information, the number of passengers in each class and the proportion (or relative frequency).\n\n\n\n\nPassengers\nProportion\n\n\n\n\n1st Class\n322\n0.2452 = 24.52%\n\n\n2nd class\n280\n0.2133 = 21.33%\n\n\n3rd Class\n711\n0.5415 = 54.15%\n\n\nTotal\n1,313\n1.0000\n\n\n\nThis information can then be shown in a bar chart. Below we show a bar chart each for the number of observations (frequency) and the proportion (relative frequency).\n\n\n\nData of Titanic passengers\n\n\n\nThis diagram, simply places a bar of height equal to the appropriate frequency (left chart) or proportion (right chart) for the different classes. Notice that the bars are separated by spaces (i.e., they are isolated) which exemplifies the discrete nature of the data.\nBoth diagrams basically display the same information and in fact the only differences are the scales on the vertical axis. We see from the diagram on the left that 322 passengers traveled First Class and from the diagram on the right that the relative frequency of 1st Class Passengers is 0.25, or 25%. Similarly, 54% were 3rd Class passengers. It is a property of discrete data that we can easily add proportions of different classes to come to new meaningful categories. I.e. 75% of passengers (21% + 54%) did not enjoy First Class Luxury.\nWhether you use the frequency or the proportion diagram depends on whether it is more important to see the absolute number or the proportions of passengers in the different classes.\nAfter this example, let us look at a formal definition of relative frequency.\n\n\n\n\n\n\nNoteDefinition - relative Frequency\n\n\n\nIf a sample consists of \\(n\\) individuals (or items), and \\(m\\leq n\\) of these have a particular characteristic, denoted \\(\\mathcal{A}\\), then the relative frequency (or proportion) of characteristic \\(\\mathcal{A}\\) in the sample is calculated as \\(\\frac{m}{n}\\). The percentage of observations with characteristic \\(\\mathcal{A}\\) in the sample would be \\(\\frac{m}{n} \\times 100\\). E.g. 0.65 is equivalent to 65%.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "graphical_descriptive.html#continuous-data---histograms",
    "href": "graphical_descriptive.html#continuous-data---histograms",
    "title": "3  Graphical Descriptive Statistics",
    "section": "3.4 Continuous data - Histograms",
    "text": "3.4 Continuous data - Histograms\nAgain we refer to the Titanic Data set in which we have 1313 observations, however only for (n = 756) do we have age information. Really, age is a continuous variable (just imagine age being measured in milliseconds) but it is commonly reported in years only. The distribution of the data can be summarised, graphically, using a histogram which is constructed as follows:\n\nGroup the raw data into intervals/classes, not necessarily of the same width:\n\nthe data are continuous, so there must (in general) be no spaces between intervals\nthe number of intervals chosen is often a fine judgement: not too many, but not too few. Depending on the data 5 to 10 is often sufficient, but in the end you should choose the number of intervals to give you informative picture about the distribution of the data. Below we have chosen 8 intervals with the first being ([0,10)) where 0 is called the lower class limit and 10 is the upper class limit. The class width (of any interval) is the difference between the upper class limit and the lower class limit. For the first interval this is (10-0=10).\nrecord the frequency in each interval; i.e., record the number of observations which fall in each of the constructed intervals\n\nFor each frequency, calculate the relative frequency, which is the frequency divided by total number of observations.\nFor each relative frequency construct a number called the density (of the interval) which is obtained as relative frequency divided by class width.\n\nIn the case of the age for passengers on the Titanic this gives rise to the following grouped frequency table:\n\n\n\n\n\n\n\n\n\n\n\nwaiting time\nclass width\nmid-point\nfrequency\nrel. freq.\ndensity\n\n\n\n\n[ a,b)\n(b-a)\n(a+b)/2\n\n\n\n\n\n[ 0,10)\n10\n5\n53\n0.00701\n0.0070\n\n\n[ 10,20)\n10\n15\n96\n0.1270\n0.0127\n\n\n[ 20,30)\n10\n25\n252\n0.3333\n0.0333\n\n\n[ 30,40)\n10\n35\n168\n0.2222\n0.0222\n\n\n[ 40,50)\n10\n45\n106\n0.1402\n0.0140\n\n\n[ 50,60)\n10\n55\n54\n0.0714\n0.0071\n\n\n[ 60,70)\n10\n65\n23\n0.0304\n0.0030\n\n\n[ 70,80)\n10\n75\n4\n0.0053\n0.0005\n\n\n\nNotice that the entries in the relative frequency column sum to 1, as they should (why?). The density calculation corresponds to the concept of density in a continuous distribution. If you calculated the size of the coloured area underneath the density histogram you would find this to equal 1.\nNow we can represent these results graphically. In fact you will sometimes see the frequencies, sometimes the relative frequencies and sometimes (although rarely) the densities represented on the vertical axis. In each case, really, the same information is conveyed and which one suits your purpose depends on what element you want to stress.\n\n\n\nFrequency, relative frequency and density histogram for ages of Titanic survivors.\n\n\n\nThis graph looks fairly similar to a bar chart, but note that the bars must be connected - no spaces. This reflects the fact that the data are continuous. On this occasion we chose 8 bins/classes. You could have created fewer or more, depending on how much detail you want to convey. But, unless you think they are informative, avoid constructing intervals, which contain no observations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "graphical_descriptive.html#time-series-data---line-graph",
    "href": "graphical_descriptive.html#time-series-data---line-graph",
    "title": "3  Graphical Descriptive Statistics",
    "section": "3.5 Time-Series data - Line Graph",
    "text": "3.5 Time-Series data - Line Graph\nWhen we have time-series data we have observations for the same variable ordered in time and we want to display the data in a manner that illustrates this time aspect of the data. The way to do this is to create a line graph (or time-series graph) as in this example that uses growth rates for UK Gross Domestic Product Data (GDP). We often label this (\\(\\Delta gdp\\_{t}\\)).\n\n\n\nLine graph, GDP Growth rate (IHYN). Source: Office of National Statistics, UK.\n\n\nYou can clearly see from this image that at the end of the sample (during the 2020 pandemic year) the drop and subsequent recovery of the UK’s GDP was unprecedented.\nCreating such line graphs with two data series can be more tricky, if the two series you want to show in one graph are on different scales. For instance the level of GDP (measured in Billions of pounds) and GDP growth (measured in percentage points). The following clip explains how to handle such a situation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "graphical_descriptive.html#two-and-more-variables-line-graphs-and-scatter-plots",
    "href": "graphical_descriptive.html#two-and-more-variables-line-graphs-and-scatter-plots",
    "title": "3  Graphical Descriptive Statistics",
    "section": "3.6 Two and more variables: Line graphs and Scatter plots",
    "text": "3.6 Two and more variables: Line graphs and Scatter plots\nThe graphical summaries, introduced above, are for summarising just one variable. An interesting question is whether two (or more) characteristics/random variables are inter-related. For example, referring to the previously used GDP growth data for which we have observations from Quarter 1 1955 to Quarter 1 2013.\n\n3.6.1 Two variables\nWe can add the inflation series (\\(inf_{i}\\)) to the line graph of the GDP growth rate.\n\n\n\nLine graph, GDP Growth rate (IHYN) and Inflation (L55O). Source: Office of National Statistics, UK.\n\n\nSuch graphs can be very revealing Although in this case it is not apparent that there is an obvious correlation between GDP growth and Inflation in the UK.\nA different type of graph often used to display how two variables are related to each other is a scatter plot. Below you can see two versions of a scatter plot displaying the same data shown in the above line graph.\n\n\n\nScatter plot, GDP Growth rate (IHYN) and Inflation (L55O). Source: Office of National Statistics, UK.\n\n\nOn the left hand side you can see the plot which shows all the data from 1989 (quarter 1) to 2021 (quarter 4). Each point represents one data point. For instance look at the one data point coloured in red in the South-West corner of the left scatter plot. That one point represents the observation from 2020 (Q2) in which GDP dropped by 13.1% and annual inflation was measured to be 0.8%.\nPerhaps you can see that the scale of this graph is driven by the very large GDP drop in Q2 2020 and the subsequent large reversal of 11.8% growth in Q3 of 2020. This compresses the vast majority of the data in the centre. Sometimes it may be worth to exclude some extreme observations (here the observations from the Year 2020 and 2021 representing the Covid period). The scatter plot on the right only shows the data up to 2019. You can now see that there is perhaps a slight positive correlation between these two variables.\nThis sort of diagram should not be viewed as way of detecting the precise nature of the relationship between the two variables and, in general, a lot of common sense is required as well. Rather, it merely illuminates the simplest, most basic, relation of whether larger \\(\\Delta gdp_{t}\\) values appear to be associated with larger (or smaller) values of the \\(inf_{t}\\) variable; thereby signifying an underlying positive (respectively, inverse or negative) observed relationship between the two. This may be suggestive about the general relationship but is by no means conclusive. Nor does it inform us on general cause and effect. It is impossible to conclude whether changes in \\(inf_{t}\\) actually cause changes in variable \\(\\Delta gdp_{t}\\) or vice versa. Indeed the two variables may not be related to each other at all. Therefore care must be taken in interpreting such a diagram.\n\nFor that reason scatter plots are more common for cross-sectional data which do not have a time series dimension. To illustrate this we will revert to a new example, relating to data from the Covid-19 pandemic. Let’s say we want to compare whether the case infection rates in countries at a particular point in time (say Week 10 in 2022) are in any way related to either how rich a country is (as measured by GDP per capita) or by how much a country spends on their Health System (as measured by the % pf GDP spend on Health).\nHere is a snippet of the spreadsheet used for this exercise.\n\n\n\nTable used for scatter diagram.\n\n\nYou can see that we have observations for different countries (in rows) which are all coming from the same time (2022-10). In columns we have different variables for each country, here the 14-day Covid-19 case rate (per 100,000), the Health expenditure (as a % of GDP), the GDP per capita and the (natural) log of GDP per capita.\nWe start by comparing the Covid case rate to the Health expenditure.\n\n\n\nScatter diagram, Health expenditure as percentage of GDP and 14-day Covid case rate, as per Week 10 in 2022.\n\n\nWhat you can see from such a scatter diagram is that there are only a few countries with very high case rates and there is no apparent correlation with the percentage of Health expenditure. Below we show scatter diagrams which relate the case rate to GDP per capita. In the left panel you can see GDP p.c. on the horizontal axis, and on the right panel you can see the same information, just that on the horizontal axis we now show the log of GDP p.c..\n\n\n\nScatter diagram, GDP per capita and 14-day Covid case rate, as per Week 10 in 2022.\n\n\nIn the scatter diagram on the left hand side (x axis: GDP p.c.) you can mainly see that there are a few countries which are incredibly reach and and a whole bunch of countries which have very lo GDP p.c. (less than $2,000 per year per person). When you have a variable on one of the axis which is so skewed, it can often be difficult to see interesting patterns.\nA common tool used in such circumstances is to scale the respective axis. Here we calculate the log of GDP p.c. and then use that on the horizontal axis (right panel). This has the effect of stretching out the GDP p.c. values at the lower end. Values of $500 and $2,000 are equivalent to approximately 6.2 and 7.6 on the logarithmic scale (as \\(ln(500) = 6.21\\) and \\(ln(2000)=7.60\\)). In this scatter diagram it is more apparent that there seems to be a positive correlation between income and covid case numbers.\nHowever, we ought to be careful and should not take these case numbers at face value. It may well be that poorer countries merely detect fewer of the existing cases.\n\n\n\n3.6.2 More than two series\nWe shall illustrate another potential pitfall when comparing time series with line graphs. For this we look at some Covid infection data across a number of different countries (Argentina, China, Italy, South Africa, Sweden and the UK). Let us first look at a line chart which compares the number of Covid infections across in the timespan from the beginning of 2020 to early 2022.\n\n\n\nLine graph. Weekly Covid cases. Source: European Center for Disease Control.\n\n\nFrom this line plot you can see that during the pandemic there were significant variations in registered Covid-19 cases across countries and across time. In particular it seems as if, for instance, the UK did fairly badly and Sweden and China did particularly well in terms of keeping the number of cases low. But you would be correct in interjecting that these countries have very different population sizes and therefore it may not be correct to compare these case numbers. It is for this reason that Covid-19 infections are usually reported as a case rate (infections per 100,000).\nThe next line graph, therefore, compares these case rates.\n\n\n\nLine graph. Weekly Covid cases per 100,000. Source: European Center for Disease Control.\n\n\nWhat we learn from here is quite different. It is still apparent that the officially reported case load in China is remarkably low. Sweden, however, cannot claim to have managed to keep the case load significantly lower than the other countries. The UK continues to have relatively high case rates. When looking at these data one also needs to keep in mind that the officially reported case numbers are significantly influenced by the testing regime. The late drop in UK cases at the beginning of 2022, for instance, is partly the result of the significant reduction in testing activity. May this be a reason for South Africa showing fairly modest infection rates across the entire Covid period?\nLet’s look at the equivalent data for Deaths related to Covid-19. While there is still significant variation across countries in terms of recording these, it is widely understood that mortality statistics are more comparable.\n\n\n\nLine graph. Weekly Covid deaths per 100,000. Source: European Center for Disease Control.\n\n\nWhen comparing these to the previous graphs it is apparent that fewer people are dying from Covid in late 2021 and 2022. This is due to significant vaccination rates. We can also see that South Africa has fairly comparable death rates compared to the other countries, indicating that the earlier infection rates may well be under-reported.\n\n\n\n\n\n\nImportantExercises\n\n\n\nTo conclude this section answer the following question.\nLine graphs are an excellent tool to (multiple correct answers are possible)\n\n illustrate how a variable changes through time compare developments through time between a small number of units (like countries, states, companies, etc.) compare developments through time between many units (like countries, states, companies, etc.) to protect yourself against comparison of units of unequal size\n\n\n\nFeedback\n\nIf you use too many units in a line plot it can quickly become impossible to read. In fact the six countries we used above are almost too many. Rarely is it useful to include more than 6 series in a time series plot. And you can actually use a line plot to make graphical comparisons which are not appropriate. We did so above when we looked at the absolute value of covid cases without adjusting for population size.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "graphical_descriptive.html#additional-resource",
    "href": "graphical_descriptive.html#additional-resource",
    "title": "3  Graphical Descriptive Statistics",
    "section": "3.7 Additional Resource",
    "text": "3.7 Additional Resource\nTo make this skill a skill that will help you when you come to apply for jobs you should practice creating such graphs yourself. All the graphs shown here can easily be created in Excel. How to create various plots in Excel: Economics Study Skills Page.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "4  Numerical Descriptive Statistics",
    "section": "",
    "text": "4.1 Introduction\nInequality is one of the biggest issues societies around the world have to deal with. Some societies are organised in ways which result in income and wealth distributions which are fairly equal (such as Scandinavian countries), others however, such as the United States and many South American countries, are organised in ways which result in large income and wealth concentrations in the hands of small groups.\nInequalities arise not only in income and wealth, but also in terms of access to education and healthcare. It has also been documented that certain groups in society are not only disproportionally represented in lower income and wealth categories but also have unequal access to opportunities. It is for this reason that the issue of inequalities has lately regained prominence in the public discourse. Part of this discourse is that the field of economics has also increased its efforts to measure inequalities (for a summary refer to Our World in Data), understand the reasons for the existing inequalities and propose policies which could address inequalities which are deemed to be too extreme (for a current summary of such efforts see the IFS Deaton Review).\nA short video summary on the task taken on by the IFS Deaton Review on Inequality is available from here.\nIn the context of this statistics unit we are interested in income distributions as they are a very good example of how we use statistics. In particular we will look at different ways to summarise distributions of variables.\nHere is an image of the Income Distribution in the UK in 2019 form the ONS.\nThis is a histogram, a graphical representation of the UK income distribution. As you can see, most people in the UK earn incomes between UKP 10,000 and UKP 40,000. The word most is actually not a very well defined term. We will use better terms later. There is a fairly small proportion of people earning more than UKP 60,000 a year. The image says that the measure displayed in the image is “Equivalised household disposable income of individuals”. It is disposable because it is calculated as income after tax. Equivalised incomes take into account whether someone lives in a single household or households with multiple people, acknowledging that larger households require more income to achieve the same standard of living as smaller households.\nWhat this histogram illustrates is how incomes in the UK population is distributed. In some sense this histogram displays all there is to know about the income distribution for everyone in the UK. We could look at similar histograms for other countries and compare these, or we could look at the same image from other years for the UK to compare. However, it turns out that this is not an easy task.\nTo make comparisons between income distributions easier, it is useful to summarise the complex information in this distribution. We call such summary numbers, summary or descriptive statistics. There is no one number which can represent all the information in this distribution. But there are particular summary statistics which represent particular aspects of this distribution. In the above histogram you can already see two such summary statistics, the mean and the median income. These are both location measures or measures of the average. Other summary statistics can tell us something about the dispersion or variance.\nIn the following we will introduce different measures of location, dispersion and skewness. But these are not the only summative or descriptive statistics there are. In particular when looking at income distributions there is a lot of interest in the amount of inequality. A number of measures have been developed ot represent the amount of inequality in a particular income distribution. We will briefly discuss such measures at the end of this lesson.\nWe shall look at three categories of numerical summaries:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#introduction",
    "href": "descriptive_stats.html#introduction",
    "title": "4  Numerical Descriptive Statistics",
    "section": "",
    "text": "Figure 4.1: UK Income Distribution. Source: ONS.\n\n\n\n\n\n\n\n\n\nMeasures of Location\nMeasures of Dispersion\nMeasures of Skewness",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#location-measures",
    "href": "descriptive_stats.html#location-measures",
    "title": "4  Numerical Descriptive Statistics",
    "section": "4.2 Location Measures",
    "text": "4.2 Location Measures\nA measure of location tells us something about what a typical value from a set of observations is. We sometimes use the expression central location, central tendency or, more commonly, average. We can imagine it as the value around which the observations in the sample are distributed.\nIn the above example of the UK income distribution two such measures are the median income (UKP 29,600) and the mean income (UKP 35,900). As we will see later, the fact that the mean is larger than the median actually tells us something important about the skewness of the distribution.\n\n4.2.1 The mean\nThe simplest numerical summary (descriptive statistic) of location is the sample (arithmetic) mean:\n\\[\\begin{equation*}\n    \\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}=\\frac{(x_{1}+x_{2}+\\ldots +x_{n})}{n}.\n\\end{equation*}\\]\nIt is obtained by adding up all the values in the sample and dividing this total by the sample size. It uses all the observed values in the sample and is the most popular measure of location since it is particularly easy to deal with theoretically. You will see later that means play an important role when we come to statistical inference.\nLet’s look at an example.\n\n\n\n\n\n\nNoteExample\n\n\n\nYou get the following 5 observations: 6, 2, 8, 10, 9\nWhat is the mean? \n** Which of the following are correct formulae for the calculation of the sample mean? (Multiple correct answers are possible)**\n\nA: \\(\\bar{x}=0.2 \\cdot (6+2+8+10+9)\\) TRUEFALSE\nB: \\(\\bar{x}=\\frac{1}{5} \\cdot (6+2+8+10+9)\\) TRUEFALSE\nC: \\(\\bar{x}=(6+2+8+10+9)/5\\) TRUEFALSE\nD: \\(\\bar{x}=\\frac{1}{4}(6+2+8+10+9)\\) TRUEFALSE\nE: \\(\\bar{x}=\\frac{1}{5}\\cdot 6+\\frac{1}{5}\\cdot 2+\\frac{1}{5}\\cdot 8+\\frac{1}{5}\\cdot 10+\\frac{1}{5}\\cdot 9\\) TRUEFALSE\n\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nYou have the following information about a sample of values: 6,7,1,9,10. You also know that there were actually six observations in the sample and you know that the sample mean is 6. What is the missing observation?\n\n\n\nFeedback\n\nThe sample mean of 6 observations is 6, then the six observations add up ot 36. As the 5 given observations sum to 33, this implies that the missing information is 3.\n\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nOne way to estimate the number of insects and investigate the possible decline of the number of insects is to count insects “collected” on number plates (or to count insect splats). Read this newsstory by Sky News. As you can read there has been a large estimated decline of insects between 2004 when this study was performed first and 2021 when it was repeated. In the the original research you can also get the following pieces of information:\n\nIn 2004, 196,448 insects were sampled over 14,466 journeys comprising 867,595 miles, a splat rate of 0.238 splats per mile.\nIn 2021, 11,712 insects were sampled over 3,348 journeys comprising 121,641 miles, a splat rate of 0.104 splats per mile.\n\nWhat is the average number of insects splats found during a car journey in 2004 and 2021. (answer to 4 decimal points, 4dp, if not indicated otherwise)\nAverage in 2004 = \nAverage in 2021 = \nWhat is the average length of a journey (measured in miles) in 2004 and 2021.\nAverage in 2004 = \nAverage in 2021 = \nWhat is the average number of insects splats found per driven mile 2004 and 2021.\nAverage in 2004 = \nAverage in 2021 = \nWhy does just comparing the average splat rates per mile not give the best possible answer to the question of how much insects have declined?\n    \\begin{itemize}\n        \\item The sample in 2021 is significantly smaller than the sample in 2004.\n        \\item The car journeys in 2004 were significantly longer than those in 2021.\n        \\item Methods to measure the length of car journeys differed from 2004 (car odometer readings) to 2021 (an app calculated the journey length from satellite data)\n    \\end{itemize}\n\n The sample in 2021 is significantly smaller than the sample in 2004. The car journeys in 2004 were significantly longer than those in 2021. Methods to measure the length of car journeys differed from 2004 (car odometer readings) to 2021 (an app calculated the journey length from satellite data)\n\n\n\nFeedback\n\nThe sample size should not be such a big issue. The sample in 2021 still feels fairly substantial. But the quite different average length of journey could indicate that the type of journeys undertaken could be quite different (e.g. relatively more build up areas in 2021), and that may make a difference for the number of insects collected. Different ways to measure important aspects like journey length might matter, but is unlikely to make a uge difference.\n\n\n\n\n\n\n\n\n\nNoteSpecial case - mean of binary variables\n\n\n\nAn important special case arises when we calculate the mean of a binary variable. Let’s say you have a sample of 7 people and you are interested in whether they have completed a university education. The data in the table below represent the available information.\n\n\n\nRespondent\nUni Degree\nu\n\n\n\n\n1\nNo\n0\n\n\n2\nNo\n0\n\n\n3\nYes\n1\n\n\n4\nYes\n1\n\n\n5\nYes\n1\n\n\n6\nNo\n0\n\n\n7\nYes\n1\n\n\n\nThe last column of the table is an alternative, and very useful way to represent this information. We created a new variable, \\(u\\), which takes the value 1 for all respondents who have a completed university degree and 0 for all others. We call this a binary variable, a variable which can take one of two outcomes.\nWhen you calculate the sample mean of the new variable (\\(u\\)) you obtain\n\\[\\begin{equation*}\n    \\bar{u}=\\frac{1}{7}\\sum_{i=1}^{7} u_{i}=\\frac{1}{7} \\cdot 4 = \\frac{4}{7} = 0.5714\n\\end{equation*}\\]\nThis is the sample mean of the variable \\(u\\) but also the proportion of respondents with a university degree. Here, and for future use, it is important that the sample proportion can be thought of as a sample mean (of the variable \\(u\\)).\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nWhat is the proportion of respondents who do not have a completed university degree?\n\n\n\nFeedback\n\nAs this is a binary variable this will be 1 - 0.5714. But it would also be the average calculated as follows: \\(\\frac{1}{7}\\sum_{i=1}^{7} (1-u_{i})=\\frac{1}{7} \\cdot 3 = \\frac{3}{7} = 0.4286\\).\n\n\n\n\n\n4.2.2 The median\nAnother measure, with which you may be familiar, is the sample median. This does not use all the values in the sample and is obtained by finding the middle value in the sample, once all the observations have been ordered from the smallest value to the largest. Thus, \\(50\\%\\) of the observations are larger than the median and \\(50\\%\\) are smaller.\n\n\n\n\n\n\nNoteExample\n\n\n\nConsider the following sample of values: 6, 2, 8, 10, 9\nWhat is the mean? \nWhat is the median? \n\n\nFeedback\n\nYou get the median by re-arranging the values in ascending order and then identify the middle value, here: 2, 6, 8, 9, 10. As we have 5 observations the middle observation is the 3rd, hence 8.\n\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nWhat to you do if you have an even number of observation? Say: 3, 9, 3, 4, 6, 7.\\\nYou still line all the observations up in ascending order: 3, 3, 4, 6, 7, 9. Now you have two middle observation, the 3rd and the 4th, here these are 4 and 6. It is then the convention that we take the midpoint between these two values to get the median.\nWhat is the mean? \nWhat is the median? \n\n\nFeedback\n\nSo here the midpoint between 4 and 6 is 5. But really, any value between 4 and 6 meets the above definition of the median. Take for instance 4.5. 50% of observations are smaller than 4.5 and 50% are larger than 4.5. Therefore 4.5 also meets the definition of the median. But here we shall stick to the convention of using the average of the two middle values. (See the section on percentiles below for a follow up on this point)\n\n\n\nAs you can see from the above examples, to calculate the mean we use all the data in the sample. But for the median we only use the middle (or the two middle) observations. The median does not use all the data and is less influenced by extreme values (or outliers), compared to the sample mean. For example, when investigating income distributions it is found that the mean income is typically higher than the median income. You can see that also from the above graphical representation of the UK income distribution, where the mean income is around UKP 6000 larger than the median.\n\n\n\n\n\n\nImportantExercise\n\n\n\nReturn to the previous exercise with sample of values: 6, 2, 8, 10, 9. For these values the sample mean was 7 and the median was 8. Let us assume that the largest of these values is actually a 100 not a 10. What are the mean and median in this case?\nWhat is the mean? \nWhat is the median? \nImagine the highest \\(10\\%\\) of earners in a country double their income, but the income of everyone else remains constant. What effect will that have on the mean on the one hand, and the median on the other?\nThe mean will increasenot changedecrease\nThe median will increasenot changedecrease\n\n\n\n\n4.2.3 Weighted mean\nIn some situations, it makes more sense to use a weighted sample mean, rather than the arithmetic mean. Before we look at what a weighted mean does we shall reformulate the arithmetic mean to the following equivalent formula:\n\\[\\begin{eqnarray*}\n    \\bar{x}&=&\\frac{1}{n}\\sum_{i=1}^{n}x_{i}=\\frac{(x_{1}+x_{2}+\\ldots +x_{n})}{n}\\\\\n    &=&\\frac{1}{n} x_{1}+\\frac{1}{n}x_{2}+\\ldots +\\frac{1}{n}x_{n}\n\\end{eqnarray*}\\]\nYou can see from the last form that the mean is a weighted sum of all observations where each observation obtains the same weight of \\(\\frac{1}{n}\\).\nLet’s say you want to figure out what the Covid incidence in Belgium, France, Luxembourg and Netherlands is in Week 10 of 2022. You have the following table with observation (drawn from spreadsheet, Covid_ECDC.xlsx, sheet: CrossSection).\n\n\n\nCountry\nWeek\nCase Rate\nPopulation\n\n\n\n\nBelgium\n2022 - Week 10\n879.8\n11,566,041\n\n\nFrance\n2022 - Week 10\n1192.0\n67,656,682\n\n\nLuxembourg\n2022 - Week 10\n1726.2\n634,730\n\n\nNetherlands\n2022 - Week 10\n4795.3\n17,475,415\n\n\n\nLet’s say you calculate the arithmetic mean for the case rate (14-day average, calculated per 100,000 population) for these countries.\n\\[\\begin{equation*}\n    \\bar{x}=\\frac{1}{4}\\sum_{i=BEL}^{NED}x_{i}=\\frac{1}{4} \\cdot 879.8 + \\frac{1}{4} \\cdot 1192.0 +\\frac{1}{4} \\cdot 1726.2 +\\frac{1}{4} \\cdot 4795.3  = 2,148.3\n\\end{equation*}\\]\nSo the mean value is 2,148.3. Is that the case rate in the four countries taken together? \\ No it is not. In the above calculation we weighted the data for France in the same way as that of Luxembourg although the latter only has about 1% of France’s population. If we wanted to use the above data to calculate what the Case rate across the four countries is we should take the different country sizes into account. Let’s supplement the above table with a new column which calculates the proportion of population each of the four countries has of the combined population (97,332,868).\n\n\n\n\n\n\n\n\n\n\nCountry\nWeek\nCase Rate\nPopulation\nPop prop, \\(w_i\\)\n\n\n\n\nBelgium\n2022 - Week 10\n879.8\n11,566,041\n0.119\n\n\nFrance\n2022 - Week 10\n1192.0\n67,656,682\n0.695\n\n\nLuxembourg\n2022 - Week 10\n1726.2\n634,730\n0.006\n\n\nNetherlands\n2022 - Week 10\n4795.3\n17,475,415\n0.180\n\n\n\nAs you can see, France has 69.5% (0.695) of the combined population and Luxembourg only 0.6% (0.006). So when we calculate the case rate across the four countries we should weigh France much higher. Note that the weights do sum to 1!\nThe way in which we calculate a case rate which then represents the (population weighted) average across the four countries is\n\\[\\begin{equation*}\n    \\bar{x}_w=0.119 \\cdot 879.8 + 0.695 \\cdot 1192.0 +0.006 \\cdot 1726.2 +0.180 \\cdot 4795.3  = 1,806.659\n\\end{equation*}\\]\nTherefore the population weighted average is 1,806.659, a fair bit lower than the unweighted average.\nLet’s look at the general formula for the weighted average:\n\\[\\begin{equation*}\n    \\bar{x}_w=\\sum_{i=1}^{n}w_{i}x_{i}=w_{1}x_{1}+w_{2}x_{2}+\\ldots+w_{n}x_{n}.\n\\end{equation*}\\]\nwhere the weights \\(\\left( w_{1},\\ldots ,w_{n}\\right)\\) satisfy \\(\\sum_{i=1}^{n}w_{i}=1\\). Note that equal weights of \\(w_{i}=n^{-1}\\), for all \\(i\\), gives the arithmetic mean. This type of average statistic if often used in the construction of index numbers (such as the Consumer Price Index, CPI) and it comes into play whenever we have data from different categories that are of different importance (as countries in the above example). In the calculation of the CPI this is relevant as price increases for food items are more important than price increases for music, as the purchases for the latter use a smaller proportion of a typical income.\n\n\n\n\n\n\nImportantExercise\n\n\n\nConsider the following data on the youth unemployment rate in the following cities/towns\n\n\n\nCity\nUnemployment Rate\nPopulation (18-25 yrs)\n\n\n\n\nSpringfield\n29.9\n6,241\n\n\nDuckburg\n11.0\n452,189\n\n\nNingzhou\n13.9\n7,457,632\n\n\nGotham City\n16.4\n13,298,115\n\n\n\nWhat is the arithmetic (unweighted) mean? \nWhat is the (population) weighted mean? \n\n\nNote that in the above examples we used populations as the weighting variable. We also mentioned the calculation of a price index. In that example the weighting variable would be something like budget shares (food - high weight, music - low weight). The appropriate weighting variable depends on what you want to calculate.\n\n\n4.2.4 Summary of average measures\nAll the above measures of location can be referred to as an average. One must, therefore, be clear about what is being calculated. Two politicians may quote two different values for the “average income in the U.K.”; both are probably right, but are computing two different measures!\nThere is another important difference between the arithmetic average and the median. The arithmetic average is really only applicable for continuous data, while the median can also be applied to discrete data which are ordinal (i.e. have a natural ranking). This makes the median applicable to a much wider range of data.\nYou may now ask what measure of central tendency is to be applied for nominal data (categorical data where the categories have no natural ordering). Neither the arithmetic average nor the median are applicable. The statistic to be used here is called the mode and it is that category that is represented the most amongst your dataset. Say you have 100 observations of which 55 are female and 45 male. Here the mode category for the gender variable is female.\nOne last note. Any of our statistics is based on a set of \\(n\\) observations. This language assumes that the observations we have is a sample only. The statistic \\(\\bar{x}\\) is therefore often also called the sample mean. As we are usually interested in the mean of the entire population we use the sample mean as an estimate of the unknown population mean (which is often represented by \\(\\mu\\)). Using \\(\\bar{x}\\) to learn about \\(\\mu\\) is what we call inference and is an important later topic.\n\n\n\nPopulation mean and sample mean.\n\n\n\n\n4.2.5 Percentiles\nSo far all measures of location introduced are in some way measures of the typical, middle or average value. But we can think of other locations in a distribution. Consider the median. We lined up all the values (or in Excel you would sort the values) according to their size. Then you find a value which splits this line-up into half, or 50% smaller and 50% larger. That value was called the median. We choose 50% on either side as we were looking for something like the middle or average value.\n\n\n\n50th Percentile.\n\n\nThat is not always what we want. In particular when looking at income distributions we are often interested in other percentage splits. For instance we may want to know what the income is which 20% of the population do not exceed. Or in other words the income where 20% earn less and 80% earn more. We shall call this the 20th percentile. Or we may be interested in the income which only 1% of the population exceed (or 99% earn less), the 99th percentile. The Occupy movement actually adopted “We are the 99%” as a political slogan.\nThe median is therefore identical to the 50th percentile.\n\n\n\nPercentiles.\n\n\nAll these percentiles are calculated in the same manner, starting with a line-up of all observations in order of their size. Then find a value which splits the data as requested.\n\n\n\n\n\n\nImportantExercise\n\n\n\nLet’s consider the following sample with 10 observations, which are already in ascending order:\\ 15, 18, 23, 28, 30, 35, 39, 48, 59, 80\nWhat is the 50th percentile?\n50th percentile = \n\n\nFeedback\n\nYou may remember from the discussion of the median, that actually any value between 30 and 35 could be the median, but that the convention is, for the median, to use the midpoint between 30 and 35.\n\n\n\nBefore we move to calculate other percentiles, let us first ask what percentile, for example, 18 is. 18 is one of the observations in the sample. There is one observation which is lower than 18 and eight which are larger. Therefore, \\(1/(1+8)=1/9=0.111111\\) or 11.1111% of observations are lower than 18.\nWhat percentile is 23? \\(2/(2+7)=2/9=0.222222\\) or 22.2222% of observations are lower than 23. Think of these as exact percentiles. There is no other value which could be this percentile.\nSo when we calculate the 20th percentile, then we know that the 20th percentile is somewhere between 18 and 23. If you again took the midpoint between the two observations you would get 20.5 and indeed you would have 20% of observations below (only 15 and 18) and 80% of observations above (all eight from 23 to 80). But 20.5 is not unique. 21.1 would have the same property. In fact for any number between 18 and 23 you could say that 20% of observations are below and 80% of observations are above. The convention to calculate such a percentile is as follows:\n\nCalculate how, relatively close the the percentile is to the two nearest exact percentiles: \\[\\begin{equation*}\n      \\frac{0.2-0.1111}{0.2222-0.1111}=\\frac{0.2-(1/9)}{(2/9)-(1/9)}= 0.8\n  \\end{equation*}\\] So the 20th percentile is at 80% of the distance between the 11.1111th percentile and the 22.2222th percentile.\nWhich value is 80% of the distance between the 11.1111th percentile (which is 18) and the 22.2222th percentile (which is 23)? \\(18+0.8 \\cdot (23-18) = 22\\). This is the 20th percentile.\n\nIn a similar way we are calculating the 90th percentile. We know that it has to lie between 59 and 80. We know that as 88.89% (=\\(8/9\\)) of observations are smaller than 59, and therefore the 90% percentile has to be between 59 and 80, as 80 is the largest observation and hence the 100th percentile. Where is the 90th percentile relative to the 88.89th and the 100th percentile (which is here the largest observation of 80)? \\((0.9-(8/9))/(1-8/9)= 0.1)\\). It is 10% the distance between 59 and 80. Therefore the 90th percentile is \\(59 + 0.1 \\cdot (80-59) = 61.1\\).\nYou can do this with any percentile.\n\n4.2.5.1 Example\nWhat is the 27th percentile?\n\\(23 + (0.27-(2/9))/(1/9) \\cdot (28-23)= 23 + 0.43 \\cdot 5 = 25.15\\)\n\n\n4.2.5.2 Percentile calculation in Excel and in Programming Languages\nThe algorithm described above is exactly what softwares like Excel, R and Python do when they calculate percentiles. This is why we follow this convention. In Excel you can actually find two functions to calculate a percentile. The function you should use is the “PERCENTILE.INC” function.\n\nThere are a few special percentiles which you may come across at times and it is good to recognise these names.\n\n\n\nPercentile\nAlternative name\n\n\n\n\n10th\nBottom/Lower decile\n\n\n20th\nBottom/Lower quintile\n\n\n25th\nBottom/Lower quartile\n\n\n50th\nMedian\n\n\n75th\nTop/Upper quartile\n\n\n80th\nTop/Upper quintile\n\n\n90th\nTop/Upper decile\n\n\n\n\n\n4.2.6 Further Reading\nThe Khan Academy has a number of good resources:\n\nIntro to Measures of Central Tendency\nAn example for mean , median and mode calculation\nA short discussion of the relation between sample and population mean",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#dispersion",
    "href": "descriptive_stats.html#dispersion",
    "title": "4  Numerical Descriptive Statistics",
    "section": "4.3 Dispersion",
    "text": "4.3 Dispersion\nA measure of dispersion (or variability) tells us something about how much the values in a sample differ from one another. The easiest measure of dispersion is called the range. All it measures is the difference between the largest and the smallest value in your observations. While this can be an informative piece of information, it has many shortcomings. First and foremost that it is influenced by outliers. Further, it only uses the two most extreme observations in the calculation. We therefore ignore a whole lot of information.\nWhat we are really after is a measure of how closely your observations are distributed around the central location.\nLet’s start with a very simple example, two samples of data (\\(X\\) and \\(Y\\)) with two observations each. For both samples the sample mean is identical, \\(\\bar{x}=\\bar{y}=4\\).\n\n\n\n\n\n\n\n\n\n\n\n\nSample X\n\\(x_i\\)\n\\(d_i=x_i-\\bar{x}\\)\n\nSample Y\n\\(y_i\\)\n\\(d_i=y_i-\\bar{y}\\)\n\n\n\n\nObs 1\n6\n2\n\nObs 1\n8\n4\n\n\nObs 2\n3\n-1\n\nObs 2\n3\n-1\n\n\nObs 3\n3\n-1\n\nObs 3\n1\n-3\n\n\n\nThe columns \\(x_i-\\bar{x}\\) and \\(y_i-\\bar{y}\\) are the deviations from the respective arithmetic mean:\n\\[\\begin{equation*}\n    d_{i}=x_{i}-\\bar{x}\n\\end{equation*}\\]\nThis measure is at the core of most measures of dispersion. It describes how much a value differs from the mean. The following image illustrates how to think of these deviations.\n\n\n\nDeviations from the mean for Samples X and Y.\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nLooking at the above Table, which of the two samples are more dispersed?\nAnswer: \n\n\nFeedback\n\nOn average the observations from Sample Y are further away from the mean than those in Sample X.\n\n\n\nWhat we want is a statistic, one value, which tells us something about the dispersion of the observations. As an average measure of deviation from \\(\\bar{x}\\), we could consider the arithmetic mean of deviations, but this will always be zero. Confirm that for Sample X and Sample Y, but it will actually always be true. In other words the positive and negative deviations from \\(\\bar{x}\\) (or \\(\\bar{y}\\)) will always cancel each other out.\nThe key to understanding this is that, in terms of dispersion, it does not matter whether the deviation \\(d_i\\) is positive or negative. We need to find ways to summarise the extend of deviation. We could either look at the absolute deviation \\(\\left| d_{i}\\right|\\) (leading to the Mean Absolute Deviation - MAD - below) or at the squared deviation, \\(d_{i}^2\\) (leading to Mean Squared Deviation - MSD. Based on these two measures of dispersion the following two statistics are available:\n\nMAD: \\(\\frac{1}{n}\\sum_{i=1}^{n}\\left| d_{i}\\right| =\\frac{1}{n}\\sum_{i=1}^{n}\\left| x_{i}-\\bar{x}\\right| &gt;0\\)\nMSE: \\(\\frac{1}{n}\\sum_{i=1}^{n}d_{i}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}&gt;0\\)\n\nThe MSD is easier to work with and lends itself to theoretical treatment. A more commonly used name for MSD is the variance. As you can see from the formula, it is calculated on the basis of \\(n\\) observations. If the data represent your relevant population then the formula for MSD is referred to as the population variance and it is often represented by \\(\\sigma^2\\). If your observations represent a sample, then the formula for MSD is changed by replacing the factor \\(1/n\\) with \\(1/(n-1)\\):\n\\[\\begin{equation*}\n    s^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}\n\\end{equation*}\\]\nThis is what is usually called the sample variance, abbreviated with \\(s^2\\). The slightly changed factor is due to ensures that \\(s^2\\) is an unbiased estimator for the unknown \\(\\sigma^2\\) which is the term we are usually interested in. In a later topic we will talk about this property of unbiasedness. For now, just accept this.\n\n4.3.0.1 Example\nCalculate the sample variance of samples X and Y:\\\nSample X: \\(s^2=\\) \nSample Y: \\(s^2=\\) \n\n\n\n\n\n\n\nNoteDegrees of Freedom\n\n\n\nTo calculate the sample variance we divide the sum of the squared deviations by \\(n-1\\) and not by the sample size \\(n\\). We call \\(n-1\\) the degrees of freedom. Degrees of freedom are a concept that we will encounter frequently when it comes to statistical inference.\nConsider Sample X above. We had three observations. We used the three observations to calculate the sample mean \\(\\bar{x}=4\\) which we then, in turn, need to calculate \\(s^2\\). Think about the following, you want to change sample values in a way which delivers the same sample mean. You changed two already, observation 1 you changed from 6 to 8 and observation 2 you changed from 3 to 4. What could the third observation be? Well in fact you cannot chose, it has to be 0. The only way to have a sample of three where two observations are 8 and 4 and the sample mean is 4 is for the third observation to be 0. So you actually only had 2 (or 3-1) degrees of freedom.\n\n\nThe variance measures have two major disadvantages:\n\nThe value you get for either \\(\\sigma^2\\) or \\(s^2\\) has no easy interpretation. This is obvious when you think about the units in which the variance is measured. If your data are, say, income data, then the unit of the variance is UKP\\(^2\\). But what is the meaning of a squared pound sterling.\nThe variances for different data sets are almost impossible to compare. The reason for that is best seen by realising that the value of the variance will change if you multiply each value by 2. However, if we multiply each observation by two, then really, nothing has changed about the dispersion of our data.\n\nIn order to address the first of these shortcomings we often refer to a different measure of dispersion, the standard deviation. This is related to the variance. In fact, once you calculated the variance you can obtain the standard deviation by taking the square root of your calculated variance\n\nPopulation standard deviation: \\(\\sigma=\\sqrt{\\sigma^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}}\\)\nSample standard deviation: \\(s=\\sqrt{s^2}=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}\\)\n\n\n\n\nVariance and standard deviation.\n\n\nOnce you calculated your sample (or population) standard deviation, you can say that the average deviation of your observation from the mean is \\(s\\) (or \\(\\sigma\\)). As the variance is always a positive measure, so is the square root of the variance, the standard deviation.\n\n\n\n\n\n\nImportantExercise\n\n\n\nYou have the following five observations:\n\n\n[1] 47  9 55 75 82\n\n\nAssuming that this is the population, calculate the following (all to 4 dp):\nMean, \\(\\mu\\) = \nVariance, \\(\\sigma^2\\) = \nStandard deviation, \\(\\sigma\\) = \nAssuming that these observations are a sample, calculate the following (all to 4 dp):\nMean, \\(\\bar{x}\\) = \nVariance, \\(s^2\\) = \nStandard deviation, \\(s\\) = \nA veterinarian weighed a sample of 6 puppies. Here are each of their weights (in kilograms): 1, 2, 7, 7, 10, 15.\nThe mean of these weights is  \\(kg\\)\nThe variance is  \\(kg^2\\)\nThe standard deviation is  \\(kg\\)\n\n\nFeedback\n\nSolution video, YouTube (7min).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#measures-of-skewness-and-higher-order-moments",
    "href": "descriptive_stats.html#measures-of-skewness-and-higher-order-moments",
    "title": "4  Numerical Descriptive Statistics",
    "section": "4.4 Measures of skewness and higher order moments",
    "text": "4.4 Measures of skewness and higher order moments\nEspecially when we think back to the example of the income distribution, we recognise that the income distribution is not symmetric. Here is an image of a symmetric, left/negative skewed and right/positive skewed distribution.\n\n\n\nSymmetric and skewed distributions.\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nThe income distribution in the UK is symmetricright skewedleft skewed.\n\n\nIt is important to recognise that the relation between the mean and the median tells us something about the skewness of a distribution. This is best illustrated with the following images.\n\n\n\nSymmetric and skewed distributions.\n\n\nWhen you are dealing with a symmetric distribution you will find the mean and the median to be the same. When we are dealing with sample data they may not be identical but very similar. The next image displays a typical left skewed distribution.\n\n\n\nLeft Skewed distribution.\n\n\nHere you can see that the mean is smaller than the median. We are having a few values of our random variable which are very small and they “drag down” the mean, but do not effect the median so much.\n\n\n\n\n\n\nImportantExercise\n\n\n\nIn the UK income distribution the mean income is smaller thanequallarger than the median income?\n\n\nThere are formal measures of skewness. We shall not introduce these in detail here. Not surprisingly one popular measure is based on comparing the sample mean and median.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#measures-of-inequality",
    "href": "descriptive_stats.html#measures-of-inequality",
    "title": "4  Numerical Descriptive Statistics",
    "section": "4.5 Measures of inequality",
    "text": "4.5 Measures of inequality\nLet us briefly return to the example of income inequality we started this lesson with. As you know, inequality is a major political and societal issue with lots of economic implications. We often differentiate between income and wealth inequality. But let’s briefly think about the income distribution introduced at the beginning of the chapter and reproduced here.\n\n\n\nUK IncomeDistribution 2019.\n\n\nTwo of the descriptive statistics we introduced already speak to aspects of inequality. Firstly the variance or standard deviation is a measure of the variety of incomes we see. The larger this measure the more differences we see in incomes. The second measure which is important in the context of this discussion is the skewness. We saw that the income distribution is right (positively) skewed meaning that we have lots of incomes at the lower end and a few very large incomes.\nA lot of effort has gone into measuring the amount of inequality in different countries and across time. An excellent example of this is the Chartbook of Economic Inequality. This has been set up by Anthony Atkinson (1944-2017) who was a true giant in economic research into inequality.\nIdeally one would want to develop one descriptive statistic that expresses all there is to learn from an income distribution about inequality. But there is no one such measure. The following measures, all of them essentially descriptive statistics describing certain aspects of an income distribution, are often used:\n\nThe Gini Index. This is a measure which measures how far away from an equal distribution a particular income distribution is. Higher measures implying more inequality. In 2014 this measure was 34 in the UK, compared to 25.75 in 1980.\nIncome Shares. For instance what share to the top 1% income earners have of the total income. In the UK that is currently around 13%\nTop percentiles as percentage of median income. For instance currently in the top decile income is around 200% of the Median income.\nPercentage of people with income below a certain level. E.g. currently in the UK around 15% of people live in households with income less than 60% of the median income.\n\nHere we will not discuss the details of these measures. The following graph displays how these measures have evolved, for the UK, through time.\n\n\n\nMeasures of income inequality, UK. Source: The Chartbook of Economic Inequality.\n\n\nAs you can see, inequality generally reduced through the 20th Century until around 1980. Since, the general direction of travel was that inequality increased.\n\n\n\n\n\n\nImportantExercise\n\n\n\nFind on the website of The Chartbook of Economic Inequality the equivalent country chart for Malaysia.\nWhich of the following statements are correct? (multiple answers are possible)\n\n The general development of inequality in Malaysia (since 1950) is similar to that in the UK. The level of poverty has decreased fairly steadily. Top earners have not increased their share of income. In general Malaysia has decreasing inequality. The Gini coefficients of the UK and Malaysia indicate that the Malaysian income distribution is more equal than that of the UK.\n\n\n\nFeedback\n\nWhile the change in inequality has lately been in different directions in the UK (increasing) and Malaysia (decreasing), Malaysia has a higher Gini coefficient indicating a higher level of inequality than the UK.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#further-resources",
    "href": "descriptive_stats.html#further-resources",
    "title": "4  Numerical Descriptive Statistics",
    "section": "4.6 Further Resources",
    "text": "4.6 Further Resources\nOn the Khan Academy you can find more practice:\n\nAn example for the calculation of the population variance\nWhy do we need the sample variance?\nThese two clips illustrates nicely why the sample variance uses the factor \\(1/(n-1)\\). First: giving an intuitive explanation of why we need the factor \\(1/(n-1)\\). Second: useing a simulation to convince you.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "CorrelationRegressionCausation.html",
    "href": "CorrelationRegressionCausation.html",
    "title": "5  Correlation, Regression and Causation",
    "section": "",
    "text": "5.1 Descriptive statistics for the relation between two or more variables\nWhen we considered graphical ways to describe data we already considered the situation of having more than two variables. Very common graphical representations in such situations are line graphs (with multiple lines) when dealing with time series data or scatter plots when dealing with cross-sectional data. In particular scatter plots allowed us to visualise how two series were related to each other.\nHere we replicate a scatter plot used in an earlier section. This plot illustrates how the country level 14-day covid case rate (in 2022, Week 10) relates to the countries Health expenditure as a share of the countries GDP.\nIt is somewhat difficult to see from this plot whether there was indeed a positive relationship between these two variables or not. In fact there are three aspects of a potential relationship you may be interested in. First, is it a positive or negative relationship, second, whether it appears to be a linear or nonlinear relationship and third, how strong is that relationship. In this section you will learn how to find numerical descriptive statistics that that speak to these aspects of such a relationship.\nThis video provides a short introduction to correlation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation, Regression and Causation</span>"
    ]
  },
  {
    "objectID": "CorrelationRegressionCausation.html#descriptive-statistics-for-the-relation-between-two-or-more-variables",
    "href": "CorrelationRegressionCausation.html#descriptive-statistics-for-the-relation-between-two-or-more-variables",
    "title": "5  Correlation, Regression and Causation",
    "section": "",
    "text": "Scatter diagram, Health expenditure as percentage of GDP and 14-day Covid case rate, as per Week 10 in 2022.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation, Regression and Causation</span>"
    ]
  },
  {
    "objectID": "CorrelationRegressionCausation.html#correlation",
    "href": "CorrelationRegressionCausation.html#correlation",
    "title": "5  Correlation, Regression and Causation",
    "section": "5.2 Correlation",
    "text": "5.2 Correlation\nA commonly used measure of association is the sample correlation coefficient, which is designed to tell us something about the characteristics of a scatter plot of observations on the variable \\(Y\\) against observations on the variable \\(X\\). In particularly, are higher than average values of \\(Y\\) associated with higher than average values of \\(X\\), and vice-versa? In the context of the above example we would ask whether higher values of health expenditure (as % of GDP) are related to higher Covid infection rates.\nConsider the following data-set in which we observe the weight (\\(Y_i\\)) measured in pounds and the height (\\(X_i\\)) measured in inches of a sample of 12 people:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nWeight \\(=Y_i\\)\n155\n150\n180\n135\n156\n168\n178\n160\n132\n145\n139\n152\n\n\nHeight \\(=X_i\\)\n70\n63\n72\n60\n66\n70\n74\n65\n62\n67\n65\n68\n\n\n\nThe best way to graphically represent the data is the following scatter plot:\n\n\n\nScatter diagram, Weight and Height.\n\n\nOn this graph a horizontal line at \\(y=154.1667\\) (at the sample mean \\(\\bar{y}\\)) and also a vertical line at \\(x=66.8333\\) (at the sample mean \\(\\bar{x}\\)) are superimposed. This creates four areas. Points in the upper right quadrant are those for which the weight is higher than average and height is higher than average. One of these points (Observation \\(i=3\\) with \\(y_i=180, x_i=72\\)) is highlighted in the scatter plot along with its deviations from \\(\\bar{x}\\) and \\(\\bar{y}\\).\nPoints in the lower left quadrant are those for which weight is lower than average and height is lower than average. Since most points lie in these two quadrants, this suggests that higher than average weight is associated with higher than average height; whilst lower than average weight is associated with lower than average height. This is typical for a positive relationship between \\(X\\) and \\(Y\\). If there was no association, we would expect to see a roughly equal distribution of points in all four quadrants.\nWhile it is often straightforward to see the qualitative nature of a relationship (positive, negative or unrelated) we want a numerical measure that describes this relationship such that we can also comment on the strength of the relationship. The basis of such a measure are again the deviations from the sample mean (as for the calculation of the variance and standard deviation), but now we have two such deviations for each observation, the deviation in the \\(Y\\) variable, \\(d_{y,i}=(y_i-\\bar{y})\\), and the deviation in the \\(X\\) variable, \\(d_{x,i}=(x_i-\\bar{x})\\). These are the deviations you can see in the above Figure.\nIn the case of the third observation with \\(y_3=180\\) and \\(x_3=72\\) we can see that both values are larger than the respective sample means \\(\\bar{y}\\) and \\(\\bar{x}\\) and therefore both, \\(d_{y,i}\\) and \\(d_{x,i}\\) are positive. \\ \\(d_{y,i}=(y_i-\\bar{y})\\) = 180-154.1667 = 25.8333\\ \\(d_{x,i}=(x_i-\\bar{x})\\) = 72-66.8333 = 5.1667\nIn fact this will be the case for all observations that lie in the upper right quadrant. For observations in the lower left quadrant we will find \\(d_{y,i}\\) and \\(d_{x,i}\\) to be smaller than 0. Observations in both these quadrants are reflective of a positive relationship. We therefore need to use the information in \\(d_{y,i}\\) and \\(d_{x,i}\\) in such a way that in both these cases we get a positive contribution to our statistic that numerically describes the relationship. Consider the term \\((d_{y,i} \\cdot d_{x,i})\\); this term will be positive for all observations in either the upper right or lower left quadrant. For values in either the upper left or lower right quadrant, however, the terms \\(d_{y,i}\\) and \\(d_{x,i}\\) will have different signs and hence the term \\((d_{y,i} \\cdot d_{x,i})\\) will be negative, reflective of the fact that observations in these quadrants are representative of a negative relationship.\nIt should now be no surprise to find that our numerical measure of a relationship between two variables is based on these terms. In particular we will use what is called the sample covariance:\n\\[\\begin{equation*}\n    Cov(X,Y)=s_{X,Y}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( x_{i}-\\bar{x}\\right) \\left( y_{i}-\\bar{y}\\right) =\\frac{1}{n-1}\\sum_{i=1}^{n} d_{x,i} d_{y,i}\n\\end{equation*}\\]\nYou can see that this is the sum of \\(d_{y,i}d_{x,i}\\) divided by \\(n-1\\). The reason for dividing by \\(n-1\\) and not \\(n\\) is similar to the reasoning used for the sample variance. In fact the measure that we typically use is the :\n\\[\\begin{eqnarray*}\n    Corr(X,Y)=r_{XY}&=&\\frac{s_{X,Y} }{\\sqrt{s^2_X s^2_Y}}\\\\\n    &=& \\frac{\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( x_{i}-\\bar{x}\\right) \\left( y_{i}-\\bar{y}\\right) }{\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( x_{i}-\\bar{x}\\right)^{2}\\,\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( y_{i}-\\bar{y}\\right) ^{2}}}\\\\\n    &=& \\frac{\\sum_{i=1}^{n}\\left( x_{i}-\\bar{x}\\right) \\left( y_{i}-\\bar{y}\\right) }{\\sqrt{\\sum_{i=1}^{n}\\left( x_{i}-\\bar{x}\\right)^{2}\\,\\sum_{i=1}^{n}\\left( y_{i}-\\bar{y}\\right) ^{2}}}\\\\\n\\end{eqnarray*}\\]\nIt is the sample covariance divided by the square root of the product of the two sample variances. In the last line we merely cancelled out the \\(1/(n-1)\\) terms.\n\n5.2.0.1 Excel application\nThe calculations are best done in a Table format or using Excel.\nThis video shows the calculations by hand (YouTube, 21min)\n\nThis video shows the calculations by Excel (YouTube, 11min)\n\n\nTable illustrating the calculations required to calculate a covariance. Values rounded to 4dp.\n\n\n\n\n\n\n\n\n\n\n\n\nObs\nWeight (Y)\nHeight (X)\n\\(y_i-\\bar{y}\\)\n\\(x_i-\\bar{x}\\)\n\\((y_i-\\bar{y})^2\\)\n\\((x_i-\\bar{x})^2\\)\n\\((y_i-\\bar{y})(x_i-\\bar{x})\\)\n\n\n\n\n1\n155\n70\n0.8333\n3.1667\n0.6944\n10.0278\n2.6389\n\n\n2\n150\n63\n-4.1667\n-3.8333\n17.3611\n14.6944\n15.9722\n\n\n3\n180\n72\n25.8333\n5.1667\n667.3611\n26.6944\n133.4722\n\n\n4\n135\n60\n-19.1667\n-6.8333\n367.3611\n46.6944\n130.9722\n\n\n5\n156\n66\n1.8333\n-0.8333\n3.3611\n0.6944\n-1.5278\n\n\n6\n168\n70\n13.8333\n3.1667\n191.3611\n10.0278\n43.8056\n\n\n7\n178\n74\n23.8333\n7.1667\n568.0278\n51.3611\n170.8056\n\n\n8\n160\n65\n5.8333\n-1.8333\n34.0278\n3.3611\n-10.6944\n\n\n9\n132\n62\n-22.1667\n-4.8333\n491.3611\n23.3611\n107.1389\n\n\n10\n145\n67\n-9.1667\n0.1667\n84.0278\n0.0278\n-1.5278\n\n\n11\n139\n65\n-15.1667\n-1.8333\n230.0278\n3.3611\n27.8056\n\n\n12\n152\n68\n-2.1667\n1.1667\n4.6944\n1.3611\n-2.5278\n\n\nSum\n1850\n802\n0\n0\n2659.6667\n191.6667\n616.3333\n\n\n\nIf you calculate \\(r\\) for the above example you should obtain a value of\n\\[\\begin{equation*}\n    Corr(X,Y)=r_{XY}=\\frac{616.3333}{\\sqrt{2659.6667 \\cdot 191.6667}} = 0.8632\n\\end{equation*}\\]\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nComplete the table below and calculate, sample means, sample variances and standard deviations, the sample covariance and the correlation for the following four observations of \\(Y\\) and \\(X\\).\n\nTable for correlation calculations.\n\n\n\n\n\n\n\n\n\n\n\n\nObs\nWeight (Y)\nHeight (X)\n\\(y_i-\\bar{y}\\)\n\\(x_i-\\bar{x}\\)\n\\((y_i-\\bar{y})^2\\)\n\\((x_i-\\bar{x})^2\\)\n\\((y_i-\\bar{y})(x_i-\\bar{x})\\)\n\n\n\n\n1\n4\n10\n-4\n6\n16\n36\n-24\n\n\n2\n8\n-2\n0\n-6\n0\n36\n0\n\n\n3\n6\n7\n\n\n\n\n\n\n\n4\n14\n1\n\n\n\n\n\n\n\nSum\n32\n16\n\n\n\n\n\n\n\n\n\\(\\bar{y} =\\) \n\\(\\bar{x} =\\) \n\\(s_Y^2 =\\) \n\\(s_X^2 =\\) \n\\(Cov(X,Y) = s_{Y,X} =\\) \n\\(Corr(X,Y) = r =\\) \n\n\nA few things are worth noting with respect to the correlation coefficient:\n\nIt can be shown algebraically that \\(-1 \\leq r \\leq 1\\).\nPositive (negative) numbers represent a positive (negative) relationship and a value of 0 represents the absence of any relationship. In our Excel application example \\(r=0.8632\\) and hence the two variables display a strong positive correlation.\n\n\n\n\nCorrelation coefficient, strength and direction.\n\n\n\nThe numerator contains the sum of the discussed cross products \\(d_{y,i} \\cdot d_{x,i}=(y_i-\\bar{y})(x_i-\\bar{x})\\)\nThe term in the denominator of the equation for \\(r\\) is related to the variances of \\(Y\\) and \\(X\\). These terms are required to standardise the statistic to be between -1 and 1.\nFor the correlation the order of variables does not matter, i.e. \\(r_{XY}=r_{YX}\\).\n\nThe covariance is actually also a measure of the relationship between these two variables, but it has many of the same shortcomings as the variance (see the Descriptive Statistics lesson). Therefore we want a standardised measure (to ensure that \\(-1\\leq r \\geq 1\\). This standardisation uses the square root of the two respective variances.\nThere are two very important limitations of the correlation coefficient:\n\nIn general, this sort of analysis does not imply causation, in either direction. Variables may appear to move together for a number of reasons and not because one is causally linked to the other.\n\n\n\n\n\n\n\nCautionCorrelation, NOT Causation\n\n\n\nFor example, over the period 1945-64 the number of TV licences \\((x)\\) taken out in the UK increased steadily, as did the number of convictions for juvenile delinquency \\(\\left( y \\right)\\). Thus a scatter of \\(y\\) against \\(x\\), and the construction of the sample correlation coefficient reveals an apparent positive relationship. However, to therefore claim that increased exposure to TV causes juvenile delinquency would be extremely irresponsible.\n\n\nAnother example illustrating that correlation must not be mis-interpreted as a causal relationship is that of ice cream sales and shark attacks, illustrated in the following image.\n\n\n\nCorrelation is not causation.\n\n\nIf we were to calculate a correlation between the two series we clearly would obtain a positive correlation. But nobody would suggest that increased ice-cream sales cause shark attacks, or indeed that increased shark attacks caused higher ice cream sales.\nHere is a different discussion on this issue: Khan Academy: Do not confuse correlation with causation.\n\nThe sample correlation coefficient gives an index of the apparent linear relationship only. It assumes that the scatter of points must be distributed about some underlying straight line. This is discussed further below. However, the term relationship is not really confined to such linear relationships.\n\n\n\n\n\n\n\nCautionRelationship, NOT Linear Relationship\n\n\n\nConsider the relationship between and . If we were to plot observations for the age and income of people in the age range of 20 to 50 we will clearly find a positive relationship. However, if we were to extend the age range to 80, we would most likely see that income decreases at the upper end of the age range. Therefore there is no linear age/income relationship across the full age range and the correlation coefficient cannot be used to describe such a relationship.\n\n\nImagine drawing a straight line of best fit through the scatter of points in the above Figure (for height and weight) simply from visual inspection. You would try and make it go through the scatter, in some way, and it would probably have a positive slope. Numerically, one of the things that the correlation coefficient does is assess the slope of such a line: if \\(r&gt;0\\), then the slope of the line of best fit should be positive, and vice-versa. Moreover, if \\(r\\) is close to either 1 (or -1) then this implies that the scatter is quite closely distributed around the line of best fit. What the correlation coefficient doesn’t do, however, is tell us the exact position of line of best fit. This is achieved using regression analysis.\n\n\n\nScatter plots and correlation.\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nConsider the following Figure which displays six scatterplots\n\n\n\nScatter plots of six bivariate data samples.\n\n\nMatch the correlations to the plots with the correlations. Note that the scales are identical for all plots.:\n\\(r_1 = 0.9226\\): Plot  \\(r_2 = -0.8835\\): Plot  \\(r_3 = 0.9931\\) Plot  \\(r_4 = -0.9936\\) Plot  \\(r_5 = -0.1988\\) Plot  \\(r_6 = 0.4813\\) Plot \n\n\nFeedback\n\nThe sign of the correlation tells you whether the correlation is positive or negative. The strength of the correlation tells you how close the points are to a straight line (not shown here but in the next section).\nIt is not straightforward to tell the plots for the last two correlations apart. Plot 6 shows some more points in the top right quadrant and hence is the one with the positive correlation.\n\n\n\n\n5.2.1 Additional resources\n\nKhan Academy: Do not confuse correlation with causation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation, Regression and Causation</span>"
    ]
  },
  {
    "objectID": "CorrelationRegressionCausation.html#regression",
    "href": "CorrelationRegressionCausation.html#regression",
    "title": "5  Correlation, Regression and Causation",
    "section": "5.3 Regression",
    "text": "5.3 Regression\nWhen thinking about correlations you learned that correlation values close to 1 or -1 imply that the points will lie close to an imaginary line, the “line of best fit”. The following image shows the six scatter plots we looked at above but now including the lines of best fit.\n\n\n\nScatter plots of six bivariate data samples and lines of best fit.\n\n\nThe lines of best fit drawn on the scatter can be represented algebraically as \\(a+bx\\). Here \\(x\\) represents the value on the horizontal axis, \\(a\\) is the intercept (i.e. the value on the vertical axis at \\(x=0\\)) and \\(b\\) is the slope (i.e. the value by which the line increases as we increase \\(x\\) by one unit). The line is defined at any value of \\(x\\) and not only those at which we have actual observations. Here you can see the line of best fit for for sample 1 with an indication of the intercept (\\(a\\)) and the slope (\\(b\\)). For the data in Sample 1 these values are \\(a=-0.34\\) and \\(b=1.1327\\). Just take these as given for now, you will shortly learn how to calculate these.\n\n\n\nScatter plot for Sample 1 and line of best fit (red).\n\n\nWhen you substitute any of the observed values \\(x_i\\) into the line of best fit you get \\(\\widehat{y}_i=a+bx_i\\). It is important to note that the result of this operation, \\(\\widehat{y}_i\\) is not the same as \\(y_i\\). Let us illustrate this in the above scatter plot. One of the scatter points is for \\(x_i=6\\) and \\(y_i=9\\) (the point is highlighted in red in the next plot.) If we plug in the values for \\(a\\) and \\(b\\), we get\n\\[\\begin{equation*}\n    \\widehat{y}_i=a+bx_i = -0.34 + 1.1327 \\cdot 6 = 6.4562\n\\end{equation*}\\]\nThe difference between the two is what is often called the residual:\n\\[\\begin{equation*}\n    res_i=y_i-\\widehat{y}_i =  y_i - (a + bx_i) = y_i - a - bx_i.\n\\end{equation*}\\]\nHere that residual is \\(res_i=y_i-\\widehat{y}_i =9-6.4562 = 2.538\\).\n\n\n\nScatter plot for Sample 1, line of best fit and example residual.\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nCalculate the residual for the following two observations:\n\n\\(res_1\\) for \\(x_1=2\\) and \\(y_1=3\\)\n\\(res_2\\) for \\(x_2=8\\) and \\(y_2=7.1\\) which are both points in the above scatter diagram.\n\nYou will note that one residual is positive and the other negative. What do these different signs represent in the scatter diagram?\n\n\n\nScatter plot for Sample 1, line of best fit and example residual.\n\n\n\\(res_1 =\\) \n\\(res_2 =\\) \nA positive sign for the residual implies that the actual observation lies belowonabove the line of best fit.\n\n\nFeedback\n\n\\[\\begin{equation*}\n    res_1=3-\\widehat{y}_1 = 3 + 0.34 - 1.1327 \\cdot 2 = 3-1.9254 = 1.0746\n\\end{equation*}\\]\nand\n\\[\\begin{equation*}\n    res_2=7.1-\\widehat{y}_2 = 7.1 + 0.34 - 1.1327 \\cdot 8 = 7.1-8.7216 = -1.6220\n\\end{equation*}\\]\nIf the residual is positive this implies that \\(y_i&gt;\\widehat{y}_i\\) and hence that the point \\(y_i\\) is above the regression line. If the residual is negative this implies that \\(y_i&lt;\\widehat{y}_i\\) and hence that the point \\(y_i\\) is below the regression line.\n\n\n\nAbove we used the relationship \\(res_i= y_i - a - bx_i\\) to calculate a residual. Re-arranging this delivers the equation which we will typically use to describe a regression relationship:\n\\[\\begin{equation*}\n    y_i = a + b x_i + res_i\n\\end{equation*}\\]\nwhere \\(a\\) and \\(b\\) represented the intercept and slope coefficients for a particular line of best fit arising from a particular sample (which is why we call \\(a\\) and \\(b\\) sample estimates - but more on this in the inference section of the course). If we want to write this relationship in a general way, i.e. not specialised on a particular sample, but for the population of our data, then we write:\n\\[\\begin{equation*}\n    y_i = \\alpha + \\beta x_i + \\epsilon_i\n\\end{equation*}\\]\nHere we have replaced the \\(a\\) and \\(b\\) with \\(\\alpha\\) and \\(\\beta\\) and the residuals \\(res_i\\) with \\(\\epsilon_i\\). \\(\\alpha\\) and \\(\\beta\\) represent the unknown values of the intercept and slope parameters that describe a liner relationship between \\(y_i\\) and \\(x_i\\) in the population. The error terms \\(\\epsilon_i\\) now represent error terms acknowledging that, even in the population, the linear relationship will not precisely represent the data.\nOnly once we have a sample of data (as in the above table) we are able to find a line of best (described by \\(a\\) and \\(b\\)). We should note that this line (and hence the \\(a\\) and \\(b\\) values) is unique to the particular sample. A slightly different sample would have delivered different values for \\(a\\) and \\(b\\). (Note: This is not unique to a regression relationship. Assume you have a population of values for some random variable \\(m_i\\) with an unknown mean \\(\\mu_m\\). Then you take a sample of values from this population and obtain a sample mean, \\(\\bar{m}\\). Had you taken a different sample, this \\(\\bar{m}\\) would also be different.)\n\n5.3.1 Naming Conventions and Causation\nIn some sense estimating a regression model is not much different to calculating a correlation between two variables, \\(X\\) and \\(Y\\). When we did talk about correlations we also noted that \\(r_{XY}=r_{YX}\\), i.e. that the correlation between \\(X\\) and \\(Y\\) is identical to the correlation between \\(Y\\) and \\(X\\).\nWhen undertaking a regression analysis this is not the case. The variables on the left hand side and the right hand side have different functions and therefore we call them by different names, such as dependent variable (on the left) and explanatory variable (on the right).\n\\[\\begin{equation*}\n    \\underset{\\begin{array}{c}\n            \\textrm{dependent variable}\\\\\n            \\textrm{explained variable}\\\\\n            \\textrm{outcome variable}\n        \\end{array}}{y_i} = \\alpha + \\beta \\underset{\\begin{array}{c}\n        \\textrm{independent variable}\\\\\n        \\textrm{explanatory variable}\n    \\end{array}}{x_i} + \\epsilon_i\n\\end{equation*}\\]\nLet’s continue thinking about the relationship between height and weight. Does it matter whether we use height or weight as the dependent variable?\n\n\n\n\n\n\nImportantExercise\n\n\n\nWhich of the two combinations do you think is more sensible? Answer: \n\nA: Height = dependent variable and Weight = explanatory variable\nB: Weight = dependent variable and Height = explanatory variable\n\n\n\nFeedback\n\nHere it is pretty obvious that individuals do have some control over their weight, but that ultimately their weight is to some degree a function of their height. Of course their weight is not solely determined by their height (do not forget their diet and levels of activity), but the height will play a role. Therefore Weight should be the dependent variable and height the explanatory variable.\n\n\n\nThe important thing to realise here is that it is YOU who has to decide which variable is dependent/explained and which is independent/explanatory. Sometimes this will be fairly obvious, like in the height-weight example, but sometimes it is not. (Is it interest rates which determine inflation or inflation which determines interest rates!!??). In any case, it is you who will have to bring your economic knowledge to the question and decide. The software (like Excel) will calculate regressions either way.\nThis issue is related to the very big question of whether you can interpret any relationship described by a regression model as a causal relationship. In fact this is such a big question that large parts of 2nd year Econometrics courses will be devoted to this question. There you will learn that this is a fiendishly difficult (and therefore exciting!!!) question. It is for now safe to assume that any regression results you obtain do NOT describe a causal relationship.\nSo in that sense regression analysis is not much different to calculating correlations. This seems to make the practice of using names like dependent and independent variables invalid. You can use these names as long as you are aware that regression results do not automatically deliver causal results. In fact, Excel will happily calculate results for both of the following regression models\n\\[\\begin{equation*}\n    Height_i = \\alpha + \\beta~ Weight_i + \\epsilon_i\n\\end{equation*}\\]\nand\n\\[\\begin{equation*}\n    Weight_i = \\gamma + \\delta~ Height_i + \\epsilon_i\n\\end{equation*}\\]\nThey will produce different (yet related) intercept and slope coefficients which is the reason why we named them differently above. The point is, the software will not be able to tell you which of the two is the sensible way of looking at the data. It is your human understanding of the problem which tells you that only the second makes sense.\n### Estimation\nRegression analysis is the statistical technique that finds the optimal values for \\(\\alpha\\) and \\(\\beta\\) given a particular sample. We will soon see how to determine the best values for \\(\\alpha\\) and \\(\\beta\\). Let us return to the example of 12 observations with height (taking the role of \\(x\\)) and weight (in this example representing \\(y\\)) data. Here is our previous scatter plot without the line of best fit.\n\n\n\nScatter diagram, Height and Weight example.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\\textbf{Exercise}\nDraw a line which you think best fits the 12 data points (If you are looking at this on a screen take a ruler and hold it over the points). From your line, what can you say, roughly, about the intercept and slope of that line (only use eye-balling here, we will get to the precise results shortly).\nThe estimated slope coefficient will be\n\n between -3 and -1 between -1 and 1 between 1 and 5 between 5 and 10\n\nThe estimated intercept coefficient will be\n\n between -100 and -70 between -70 and -30 between -30 and 30 between 30 and 700 between 70 and 100\n\n\n\nFeedback\n\nOn the graph you cannot see \\(x=0\\). But when eye-balling the line of best fit it drops by around 60 weight units for a drop of 20 height units. That means that the slope is around 3. For a value of height = 60 the line of best fit has an approximate weight value of 130. If, for every height unit drop the weight drops by about 3, then y should drop by \\(60 \\times 3 = 180\\) as x goes from 60 to 0. Hence, the intercept \\(a\\) should be at approximately \\(130-180 = -50\\). Don’t get confused by the graph only showing x values down to 55.\n\n\n\nLet’s now calculate the actual sample estimates \\(a\\) and \\(b\\). And let’s do that precisely. But don’t forget these values will be specific to that particular sample of 12 observations. We will continue to not know what the true values \\(\\alpha\\) and \\(\\beta\\) are and, if we had a different sample we would get somewhat different values for \\(a\\) and \\(b\\).\nBefore we do so we want to point out what optimal means in this context. In fact it implies that we want to minimise the values for \\(res_i\\) for all \\(i=1,...,n\\) observations in the sample.\n\n\n\nScatter diagram with linear regression, Height and Weight example.\n\n\nIn fact what we want to minimise is the sum of squared residuals\n\\[\\begin{equation*}\n    (y_{1}-\\widehat{y}_{1})^{2}+(y_{2}-\\widehat{y}_{2})^{2}+...+(y_{12}-\\widehat{y}_{12})^{2}=\\sum_{i=1}^{n}(y_{i}-\\widehat{y}_{i})^{2} = \\sum_{i=1}^{n}(y_{i}-a-bx_{i})^{2}.\n\\end{equation*}\\]\nThis is equivalent to saying that we want to minimise the variation of our sample observations around the regression line (\\(a+bx\\)). The technique of obtaining the best values for \\(\\alpha\\) and \\(\\beta\\), which we call \\(a\\) and \\(b\\), in this way is also known as ordinary least squares (OLS) since it minimises the sum of squared deviations (sum of squared residuals) from the fitted line. We shall not dwell on the algebra here (You have a function and want to chose the \\(a\\) and \\(b\\) such that the function is minimised … two partial derivatives … setting them to 0 … solving for \\(a\\) and \\(b\\) … you know the drill), but the solutions to the algebraical problem are:\n\\[\\begin{equation*}\n    b=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}},\\quad a=\\bar{y}-b\\bar{x};\n\\end{equation*}\\]\nHere, as earlier, \\(\\bar{y}\\) and \\(\\bar{x}\\) are the sample means of variables \\(y\\) and \\(x\\). Applying the technique to the weight and height data yields \\(b=616.3333/191.6667=3.2157\\), and \\(a=154.1667-3.2157\\cdot 66.8333=-60.7491\\). All the numbers we need here were calculated in the earlier Table for this example. This gives the smallest possible sum of squared residuals as \\(677.753\\), a value which is not so easily read of the table. The regression line (line of best fit) is therefore defined as:\n\\[\\begin{equation*}\n    \\hat{y}=-60.7491+3.2157 \\times x\n\\end{equation*}\\]\nIt turns out that this line should be quite close to the line of best fit I asked you to draw earlier. As it turns out our eyes are pretty good in visual fitting. This is particularly easy if you have the sample means of \\(\\bar{y}\\) and \\(\\bar{x}\\) available, as the line of best fit will always go through the point \\({\\bar{y}, \\bar{x}}\\).\n\n5.3.1.1 Excel application\nVideo illustrating the estimation of a simple linear regression in Excel (YouTube, 6min).\n\nWhen estimating the regression by EXCEL, the results are presented as follows:\n You can recognise that the sample estimates for the intercept and the slope appear in that regression output (highlighted in yellow). We will interpret a number of further values from that output as we go along. There is one which you should already recognise: The sample size (Observations) of 12. Another one, a little less obvious is the value of 2659.667. It appears here as “SS Total” or Sum of Squares Total. Find that value in the Table from which we calculated the correlation for this data set, it is \\(\\sum_{i=1}^{12}(y_{i}-\\bar{y})^{2}\\). It measures the amount of variation we can see in the dependent variable. What we want to achieve by using regression analysis is to explain some of this variation through \\(y\\)’s linear relationship with \\(x\\).\nYou can see from the regression output that here 74.5% of that variation is explained by that linear relationship (see the “R square” statistic at the top of the output.)\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nWhich of the following are equivalent to the OLS estimator \\(b\\)? Answer TRUE if equivalent.\n\n\n\n\n\n\n\nFormula\nanswer\n\n\n\n\n\\(b=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(x_{i}-\\bar{x})}\\)\nTRUEFALSE\n\n\n\\(b=\\frac{n\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n^{-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}\\)\nTRUEFALSE\n\n\n\\(b=\\frac{n^{-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n^{-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}\\)\nTRUEFALSE\n\n\n\\(b=\\frac{(n-1)^{-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{(n-1)^{-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}\\)\nTRUEFALSE\n\n\n\\(b=\\frac{Cov(X,Y)}{s_X^2}\\)\nTRUEFALSE\n\n\n\\(b=\\frac{Cov(X,Y)~s_Y}{s_Y~s_X^2}\\)\nTRUEFALSE\n\n\n\\(b=Corr(X,Y)~\\frac{s_Y}{s_X}\\)\nTRUEFALSE\n\n\n\n\n\nHint\n\nAll but the second are equivalent. Use the definitions of sample covariance, correlation and standard deviations to confirm.\n\n\nEarlier in this lesson you calculated a number of statistics for a sample of \\(n=4\\) data points:\n\n\nRegression line calculation exercise.\n\n\nObs\nY\nX\n\n\n\n\n1\n4\n10\n\n\n2\n8\n-2\n\n\n3\n6\n7\n\n\n4\n14\n1\n\n\nSum\n32\n16\n\n\n\nThe statistics calculated were:\n\\(\\bar{y} = 8\\), \\(\\bar{x} = 4\\), \\(s_Y^2 = 18.6667\\), \\(s_X^2 = 30\\), \\(Cov(X,Y) = s_{Y,X} = -16\\), \\(r = -0.6761\\)\nPlease make sure you remember how to calculate these! Use this information to estimate the sample estimates \\(a\\) and \\(b\\), to obtain the line of best fit for the regression \\(Y_i = \\alpha + \\beta X_i + \\epsilon_i\\) (to 4dp).\n\\(b =\\) \n\\(a =\\) \n\n\nHint\n\n\\(b=\\frac{Cov(X,Y)}{s_X^2}=\\frac{-16}{30}=-\\frac{8}{15}\\)\n\\(a=\\bar{y}-b\\cdot \\bar{x}=8-(-\\frac{8}{15})\\cdot 4=\\frac{120}{15}+\\frac{32}{15}=\\frac{152}{15}\\)\n\n\n\nThere are a number of issues that need to be stressed here. The first relates, yet again, to the sample/population issue. In most cases the data available to run a regression will be sample data. Recall how we previously discussed that the \\(\\bar{x}\\) was the sample estimate for some unknown population parameter \\(\\mu\\) or the sample variance, \\(s^2\\) was the sample estimate of some unknown population variance, \\(\\sigma^2\\). In the same spirit it turns out that the values of \\(a\\) and \\(b\\) that describe the line of best fit, are sample estimates of some unknown population parameters (usually labelled, \\(\\alpha\\) and \\(\\beta\\)).\nAlso note that \\(b\\) is the slope of the fitted line, \\(\\hat{y}=a+bx\\); i.e., the derivative of \\(\\hat{y}\\) with respect to \\(x\\):\n\\[\\begin{equation*}\n    b=d\\hat{y}/dx\n\\end{equation*}\\]\nand measures the increase in \\(\\hat{y}\\) for a unit increase in \\(x\\).\nLet us look at the height and weight example for which we established the regression line to be:\n\\[\\begin{equation*}\n    \\hat{y}=-60.7491+3.2157 ~ x\n\\end{equation*}\\]\nWhen you interpret regression results it is extremely important to be aware of the units in which the dependent and explanatory variables are measured. In the case of the Height-Weight example, the explanatory variable (Height, \\(x\\)) is measured in inches (1 inch = 2.54 cm) and the dependent variable (Weight, \\(y\\)) is measured in pounds (1 pound = 1 lbs = 0.454 kg). Do not ask why pounds are abbreviated as lbs. They are.\nSo, with that knowledge, how would we interpret the estimated slope coefficient, \\(b=3.2157\\)? To interpret this we need to adapt the general formula “\\(b\\) measures the increase in \\(\\hat{y}\\) for a unit increase in \\(x\\)”. Applied to this example this implies the following:\n\n“The expected weight (\\(\\hat{y}\\)) increases by 3.2157 pounds for every height increase of 1 inch.”\n“On average weight (\\(\\hat{y}\\)) increases by 3.2157 pounds for every height increase of 1 inch.”\n\nRecall that expectations are measured with averages, hence both expressions can be used.\nThe intercept in a regression, here \\(a = -60.7491\\), sometimes, but not always, can be interpreted. Note that the expected value, when the \\(x\\) variable takes the value of 0 (\\(x=0\\)) is \\(\\hat{y}=a\\), and that is how we interpret the intercept. If someone has a height of 0 inches then we would expect the person to have a weight of -60.7491. This, of course, does not make any sense!!! There are no people with a height of 0. Interpreting the intercept only makes sense if the value of \\(x=0\\) is a sensible value and inside the sample of values of \\(x\\). In our example the smallest height is 60 inches and the largest height is 74 inches. So even interpreting the results for a height of 40 inches would not make sense as that as well would be outside the sample range.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation, Regression and Causation</span>"
    ]
  },
  {
    "objectID": "CorrelationRegressionCausation.html#transformations-of-data",
    "href": "CorrelationRegressionCausation.html#transformations-of-data",
    "title": "5  Correlation, Regression and Causation",
    "section": "5.4 Transformations of data",
    "text": "5.4 Transformations of data\nNumerically, transformations of data can affect the above summary measures. For example, in the weight-height scenario, consider for yourself what would happen to the values of \\(a\\) and \\(b\\) and the correlation if we were to use kilograms and centimetres rather than pounds and inches.\nConvert the height of people from inches to cm (1 inch = 2.54 cm), but leaving the weight unchanged, measured in lbs, what are the regression results and how would you interpret the results?\n\n5.4.0.1 Excel application\nVideo explaining the effect of re-scaling the explanatory variable (YouTube, 8min).\n{{ &lt; video https://youtu.be/aDNUHA4h4Xo &gt;}}\nThe results as presented by EXCEL are shown here:\n\n\n\nEXCEL Regression Output.\n\n\nThe estimated regression relationship is (noting that \\(x\\) is now measured in cm):\n\\[\\begin{equation*}\n    \\hat{y}=-60.7461+1.2660 ~ x\n\\end{equation*}\\]\nSo the slope coefficient is smaller than the original ones which was 3.2157. Let’s calculate the ratio of these two estimates: \\(3.2157/1.2660 = 2.54\\). As we multiplied the explanatory variable by 2.54, this resulted in the slope coefficient being divided by the same amount. That is no accident.\nWhat you basically did is best described by comparing the two regression models (giving the regression parameters different names). We start by looking at the original model (using the height measurement in inches):\n\\[\\begin{equation*}\n    Weight[lbs]_i = \\alpha + \\beta~ Height[in]_i + \\epsilon_i\n\\end{equation*}\\]\nNow we look at the regression model with height measured in centimeters but then convert it to one where height is measured in inches.\n\\[\\begin{eqnarray*}\n    Weight[lbs]_i &=& \\gamma + \\delta~ Height[cm]_i + v_i \\\\\n    Weight[lbs]_i &=& \\gamma + \\delta~ (2.54 \\cdot Height[in]_i) + v_i  \\\\\n    Weight[lbs]_i &=& \\gamma + 2.54 \\cdot \\delta Height[in]_i + v_i\n\\end{eqnarray*}\\]\nIf you now compare the coefficients you see that \\(\\alpha = \\gamma\\) and indeed the new intercept is basically identical to the originally estimated constant. Comparing the two models tells us, that the slope coefficients should be related as follows: \\(\\beta =2.54 \\cdot \\delta\\). And indeed, the original coefficient estimated (in the inches model) is 2.54 times larger than that estimated in the centimeter model.\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nConsider the following regression model:\n\\[\\begin{equation*}\n    Weight[kg]_i = \\eta + \\phi~ Height[in]_i + \\epsilon_i\n\\end{equation*}\\]\nHow are the coefficients of this model related to those in the original model? Note that 1lbs = 0.454kg.\n\nA: $ = 0.454 $ and \\(\\phi = 0.454 \\cdot \\beta\\)\nB: $ 0.454 = $ and $ 0.454 = $\nC: $ = $ and \\(\\phi = 0.454 \\cdot \\beta\\)\nD: $ = $ and $ 0.454 = $\n\nWhich of these is correct? \n\n\nFeedback\n\nWe go through a similar transformation process as in the previous example:\n\\[\\begin{eqnarray*}\n    Weight[kg]_i &=& \\eta + \\phi~ Height[in]_i + u_i \\\\\n    0.454 \\cdot Weight[lbs]_i &=& \\eta + \\phi~ Height[in]_i + u_i   \\\\\n    Weight[lbs]_i &=& \\frac{\\eta}{0.454} + \\frac{\\phi}{0.454} ~ Height[in]_i + \\frac{1}{0.454}~\\epsilon_i   \\\\\n\\end{eqnarray*}\\]\nThe last line is reformulated into the variables in their original units and hence we can compare coefficients: $ = 0.454 $ and \\(\\phi = 0.454 \\cdot \\beta\\).\n\nEstimate the following regression model in EXCEL:\n\\[\\begin{equation*}\n    Weight[kg]_i = \\eta + \\phi~ Height[in]_i + \\epsilon_i\n\\end{equation*}\\]\nConfirm that the estimated coefficients are indeed related to those in the original model as suggested in the previous exercise.\n\n\nFeedback\n\n\n\n\nEXCEL Regression Output.\n\n\nWe find the intercept to be \\(-27.5787\\) which is indeed approximately \\(0.454 \\cdot -60.7461\\) and the slope to be \\(1.4599\\) which is approximately \\(0.454 \\cdot 3.2157\\).\n\n\n\nIn the examples above, the scatter plot of the data suggested that, indeed, if there was a relationship between two variables, then that relationship was well described by a linear relationship, i.e. a relationship which graphically follows, approximately, a straight line.\nAn important matter arises if we find that a scatter of a variable \\(y\\) against another, \\(x\\), does not appear to reveal a linear relationship. In such cases, linearity may be retrieved if \\(y\\) is plotted against some function of \\(x\\) (e.g., \\(\\log (x)\\) or \\(x^{2}\\), say). Indeed, there may be cases when \\(Y\\) also needs to be transformed in some way. That is to say, transformations of the data (via some mathematical function) may render a non-linear relationship “more” linear.\nA particular interesting transformation often used for economics series is a log-log transformation, which means that both dependent and explanatory variables are a log transform. In applied economics studies of demand, the \\(\\log\\) of demand \\((Q)\\) is regressed on the \\(\\log\\) of price \\((P)\\), in order to obtain the fitted equation (or relationship). Why does this make sense?\nFor example, suppose an economic model for the quantity demanded of a good, \\(Q\\), as a function of its price, \\(P\\), is postulated as approximately being \\(Q=\\alpha P^{\\beta}\\) where \\(\\alpha\\) and \\(\\beta\\) are unknown parameters, with \\(\\alpha&gt;0\\), \\(\\beta&lt;1\\) to ensure a positive downward sloping demand curve.\nThe nice thing about such a model is that the interpretation of \\(\\beta\\) is the price elasticity of demand. Recall that this elasticity is defined as \\((dQ/Q)/(dP/P)=(dQ/dP)(P/Q)\\). From the above model you can derive that \\(dQ/dP = \\alpha \\beta P^{\\beta-1}\\). With that information we get the following for the price elasticity in that model:\n\\[\\begin{eqnarray*}\n    \\frac{dQ}{dP}\\frac{P}{Q} &=& \\alpha \\beta P^{\\beta-1}\\frac{P}{Q}\\\\\n    &=& \\alpha \\beta P^{\\beta-1}\\frac{P}{\\alpha P^{\\beta}}\\\\\n    &=&  \\alpha \\beta \\frac{P^{\\beta}}{\\alpha P^{\\beta}}\\\\\n    &=& \\beta\n\\end{eqnarray*}\\]\nWhy does that make estimating a log-log model so attractive? Take the \\(log\\) on both sides of the postulated model you get \\(\\log(Q)=\\alpha^{\\ast}+\\beta \\log(P)\\), where \\(\\alpha^{\\ast}=\\log(\\alpha)\\). Thus, if \\(n\\) observations are available, (\\(q_{i},p_{i}\\)), \\(i=1,...,n\\), a scatter plot of \\(\\log(q_{i})\\) on \\(\\log (p_{i})\\) should be approximately linear in nature. This then suggests that a simple regression of \\(\\log (q_{i})\\) on \\(\\log (p_{i})\\) would provide a direct estimate of the elasticity of demand which is given by the value \\(\\beta\\), namely the estimated slope parameter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation, Regression and Causation</span>"
    ]
  },
  {
    "objectID": "CorrelationRegressionCausation.html#additional-resources-1",
    "href": "CorrelationRegressionCausation.html#additional-resources-1",
    "title": "5  Correlation, Regression and Causation",
    "section": "6.1 Additional resources",
    "text": "6.1 Additional resources\n\nKhan Academy: Setup of the OLS problem and how to proof that the above formulae for \\(a\\) and \\(b\\) and four follow on clips - click on “up next” at the end of each clip). But be careful, in his video Salman Khan uses \\(m\\) for what we call \\(b\\) and \\(b\\) for what we call \\(a\\). Life is never easy!\nLink to a full (55min) undergraduate, introductory lecture on regression. Minutes 1 to 22 are relevant for this lesson.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation, Regression and Causation</span>"
    ]
  },
  {
    "objectID": "DealingWithDataSets.html",
    "href": "DealingWithDataSets.html",
    "title": "6  Dealing with real life datasets",
    "section": "",
    "text": "6.1 Excel\nExcel is an industry standard spreadsheet software by Microsoft. It is by far the most common spreadsheet software and if you can work in Excel you will be able to work in most other spreadsheet softwares (such as Numbers for Macs or Google Sheets). A spreadsheet organises data in tables and that is the most common way to present data.\nBeing “fluent” in Excel is absolutely critical for your time as a student and, more importantly, for your time after University. You will therefore have to learn a number of critical skills in Excel.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dealing with real life datasets</span>"
    ]
  },
  {
    "objectID": "DealingWithDataSets.html#excel",
    "href": "DealingWithDataSets.html#excel",
    "title": "6  Dealing with real life datasets",
    "section": "",
    "text": "Data Entry\nRe-arranging data into a well organised table\nSorting and filtering data\nCleaning Data\nUsing formulae / Calculate new variables\nMerging Data\nCalculating Summary Statistics\nProducing Graphs\n\n\n6.1.1 File Formats\nExcel files have extension “xlsx”. Some Excel files which have been created on old versions of Excel would have the extension “xls”.\n\n\n\nKey elements of an xlsx file.\n\n\nAnother common file formal for spreadsheets is a “csv” file. That stands for “comma separated values”. While you can open “xlsx” files only with a spreadsheet software, the advantage of a “csv” file is that it is basically just a text file in which you can find the values of the data table. For instance the same data as in the earlier image would look like this in a “csv” file:\n\n\n\nStructure of a csv file.\n\n\nIf you open a “csv” file with a spreadsheet software, the software will automatically present the data as if it was a spreadsheet. However, you cannot save any graphs or multiple tabs in “csv” files. Such files are very popular for storing large datasets, as the resulting file sizes are significantly smaller than equivalent “xlsx” file sizes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dealing with real life datasets</span>"
    ]
  },
  {
    "objectID": "DealingWithDataSets.html#a-good-data-table",
    "href": "DealingWithDataSets.html#a-good-data-table",
    "title": "6  Dealing with real life datasets",
    "section": "6.2 A Good Data table",
    "text": "6.2 A Good Data table\nData should be organised in tables. This is not a doctrine and indeed if you have very high-dimensional data (many variables and many observations) then there may well be better (more efficient) alternatives. But for the purpose of this unit we will be thinking about tables.\nHere is an example of a well structured, yet very simple table.\n\n\n\nA simple table.\n\n\nYou need to think about observations (in rows) and variables (in columns). In the example you see for instance the observation for Afghanistan. For each observation we have information (variables!) on the size of the Land Area, Health Expenditure and GDP per capita. You can also see that the country information comes in three forms (three variables): “country”, “geoID_2” and “geoID_3”. The two latter are abbreviations for countries in either two or three letter codes. These are super useful as you will see in the worked example below (see IBAN codes.\nYou can see that all information for any particular country is organised in one row (e.g. row 2 for Afghanistan, row 6 for Angola). All information for the same variable is presented in the same column (e.g. col D for the Land area and col F for the per capita GDP). This type of clean organisation is super important, but sometimes you will have to first do some work to get there.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dealing with real life datasets</span>"
    ]
  },
  {
    "objectID": "DealingWithDataSets.html#data-sources",
    "href": "DealingWithDataSets.html#data-sources",
    "title": "6  Dealing with real life datasets",
    "section": "6.3 Data Sources",
    "text": "6.3 Data Sources\nThere are many places from which you can download data. It is impossible to list them all, but a selection of good places is listed on the Economics Department list of data sources.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dealing with real life datasets</span>"
    ]
  },
  {
    "objectID": "IntroToProbability.html",
    "href": "IntroToProbability.html",
    "title": "7  Introducing Probability",
    "section": "",
    "text": "7.1 Introduction\nThis video gives a brief introduction into why economists and social scientists should study probabilities (YouTube, 4min).\n{{ &lt; video https://www.youtube.com/watch?v=I3OFG1QlDLc &gt; }}\nSo far we have been looking at ways of summarising samples of data drawn from an underlying population of interest. Although at times tedious, all such arithmetic calculations are fairly mechanical and straightforward to apply. To remind ourselves, one of the primary reasons for wishing to summarise data is so assist in the development of inferences about the population from which the data were taken. That is to say, we would like to elicit some information about the mechanism which generated the observed data.\nPerhaps you can recall this little schematic which we discussed in the introduction.\nWe now start on the process of developing mathematical ways of formulating inferences and this requires the use of probability. This becomes clear if we think back to one of the early questions posed in this course: “prior to sampling is it possible to predict with absolute certainty what will be observed?” The answer to this question is “no”; although it would be of interest to know how likely it is that certain values would be observed. Or, what is the probability of observing certain values?\nBefore proceeding, we need some more tools:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducing Probability</span>"
    ]
  },
  {
    "objectID": "IntroToProbability.html#introduction",
    "href": "IntroToProbability.html#introduction",
    "title": "7  Introducing Probability",
    "section": "",
    "text": "Schematic of Statistical Inference and how it relates to descriptive statistics and probability calculus.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducing Probability</span>"
    ]
  },
  {
    "objectID": "IntroToProbability.html#venn-diagrams",
    "href": "IntroToProbability.html#venn-diagrams",
    "title": "7  Introducing Probability",
    "section": "7.2 Venn diagrams",
    "text": "7.2 Venn diagrams\nVenn diagrams (and diagrams in general) are of enormous help in trying to understand, and manipulate probability. We begin with some basic definitions, some of which we have encountered before.\n\nExperiment: any process which, when applied, provides data or an outcome; e.g., rolling a dice and observing the number of dots on the upturned face; recording the amount of rainfall in Manchester over a period of time.\nSample Space: set of possible outcomes of an experiment; e.g., \\(S\\) (or $$) \\(=\\) \\(\\{1,2,3,4,5,6\\}\\), which is the sample space of rolling a dice. Or \\(S=\\{x;x\\geq 0\\}\\), which is the sample space of an experiment where the outcomes can be any real non-negative number, or the set of real non-negative real numbers.\nSimple Event: just one of the possible outcomes on \\(S\\)\nEvent: a subset of \\(S\\), denoted \\(E\\subset S\\); e.g., \\(E=\\left\\{ 2,4,6\\right\\}\\) (i.e. any even number on a dice) or \\(E=\\left\\{ x;4&lt;x\\leq 10\\right\\}\\), which means “the set of real numbers which are strictly bigger than \\(4\\) but less than or equal to \\(10\\)”.\nNote that an event, \\(E\\), is a collection of simple events. You will have to decide what your event of interest is and that depends on the situation.\n\nSuch concepts can be represented by means of the following Venn Diagram:\n\n\n\nVenn Diagram.\n\n\nThe sample space, \\(S\\), is depicted as a closed rectangle, and the event \\(E\\) is a closed loop wholly contained within \\(S\\) and we write (in set notation) \\(E\\subset S\\).\nIn dealing with probability, and in particular the probability of an event (or events) occurring, we shall need to be familiar with unions, intersections and complements.\nTo illustrate these concepts, consider the sample space \\(S=\\{x;x\\geq 0\\}\\). In this example the sample space represents an infinite set of numbers. Don’t let the square finite sample space, represented in the next Figure, distract you from that. The following events defined on \\(S\\), as depicted in the Figure:\n\\(E=\\{x;4&lt;x\\leq 10\\},\\,F=\\{x;7&lt;x\\leq 17\\},\\,G=\\{x;x&gt;15\\},\\,H=\\{x;9&lt;x\\leq 13\\}\\).\n\n\n\nVenn Diagram.\n\n\nNote that the size of the graphical representation does not represent any probabilities here.\n\n\nEvent \\(E\\): This graphical representation merely illustrates that \\(E\\) is a subset of \\(S\\), \\(E\\subset S\\)\n\n\nUnion: \\(E\\cup F\\)\nThe union of \\(E\\) and \\(F\\) is denoted \\(E\\cup F\\), with \\(E\\cup F=\\{x;4&lt;x\\leq 17\\}\\); i.e., it contains elements (simple events) which are either in \\(E\\) or in \\(F\\) or (perhaps) in both. This is illustrated on the Venn diagram by the dark shaded area in diagram (b).\n\n\nIntersection: \\(E\\cap F\\)\nThe intersection of \\(E\\) and \\(F\\) is denoted \\(E\\cap F\\), with $EF={ x;7x} $; i.e., it contains elements (simple events) which are common to both \\(E\\) and \\(F\\). Again this is depicted by the dark shaded area in (c).\n\n\nThe Null set/event: $EG=$\nIf events have no elements in common (as, for example, \\(E\\) and \\(G\\)) then they are said to be mutually exclusive, and we can write \\(E\\cap G=\\emptyset\\), meaning the null set which contains no elements. Such a situation is illustrated on the Venn Diagram by events (the two shaded events in (d)) which do not overlap. Notice however that \\(G\\cap F\\neq \\emptyset\\), since \\(G\\) and \\(F\\) have elements in common.\n\n\nComplement of \\(E\\): \\(\\bar{E}\\)\nThe complement of an event \\(E\\), say, is everything defined on the sample space which is not in \\(E\\) This event is denoted \\(\\bar{E}\\), the dark shaded area in (e); here \\(\\bar{E}=\\left\\{ x;x\\leq 4\\right\\} \\cup \\left\\{ x;x&gt;10\\right\\}\\). An alternative notation that is sometimes used for the complement of an event \\(E\\) is \\(E^c\\).\n\n\nSubset of \\(F\\): \\(H\\subset F\\) and \\(H\\cap F=H\\)\nFinally note that \\(H\\) is a sub-set of \\(F\\); see (f). It is depicted as the dark closed loop wholly contained within \\(F\\), the lighter shaded area, so that \\(H\\cap F=H\\); if an element in the sample space is a member of \\(H\\) then it must also be member of \\(F\\). (In mathematical logic, we employ this scenario to indicate that “\\(H\\) implies \\(F\\)”, but not necessarily vice-versa.) Notice that $GH=$ but \\(H\\cap E\\neq \\emptyset\\).\n\n\nThis video goes through most elements of the above example (YouTube, 8min).\n{{ &lt; video https://www.youtube.com/watch?v=RYRze4XXlrE &gt;}}\n\n\n\n\n\n\nNoteExercise\n\n\n\nSuppose that the sample space \\(S\\) consists of the positive integers from 1 to 10 inclusive. Let \\(X\\) and \\(Y\\) be the following sets:\n\\(X=\\{2,3,4\\},\\\\\\,Y=\\{3,4,5\\},\\\\Z=\\{5,6,7\\}\\).\nList the members in the following sets. For the Emplty set answer “.”. If the answer is a single number give that number, if the answer contains several numbers, e.g. 3, 5 and 6, then asnwer “3,5,6”, numbers in ascending order , seperated by a comma and without spaces).\n\\(\\bar{X}\\cap Y =\\) \n\\(\\bar{X}\\cup Y  =\\) \n\\(X\\cap Z  =\\) \n\\(Y\\cup Z  =\\) \n\\(\\overline{Y\\cup Z} =\\) \n\\(X\\cap(\\overline{Y\\cup Z})  =\\) \n\\(\\overline{X\\cap(\\overline{Y\\cup Z})}  =\\) \n\n\nSolutions\n\n\n\\(\\bar{X}\\cap Y = \\{5\\}\\)\n\\(\\bar{X}\\cup Y  = \\{1,3,4,5,6,7,8,9,10\\}\\)\n\\(X\\cap Z  = \\{ \\emptyset \\}\\)\n\\(Y\\cup Z  = \\{3,4,5,6,7\\}\\)\n\\(\\overline{Y\\cup Z}  = \\{1,2,8,9,10\\}\\)\n\\(X\\cap(\\overline{Y\\cup Z})  = \\{2\\}\\)\n\\(\\overline{X\\cap(\\overline{Y\\cup Z})}  = \\{1,3,4,5,6,7,8,9,10\\}\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducing Probability</span>"
    ]
  },
  {
    "objectID": "IntroToProbability.html#probability",
    "href": "IntroToProbability.html#probability",
    "title": "7  Introducing Probability",
    "section": "7.3 Probability",
    "text": "7.3 Probability\nThe term probability is used in everyday conversation. Watching the daily news, for instance, you will come across weather forecasts which will often include probabilistic statements like “There is a 70% probability of rain for tomorrow.” Here is a video on weather forecasts and an incredibly interesting podcast on the importance of communicating uncertainty in weather forecasts (38 min).\nOther areas where we use probabilities are sports (What is the probability of England winning the World Cup) games of chance (e.g. roulette or poker) or economics (What is the probability of a recession?). What you will do now is to learn about a coherent theory of probability; a theory which allows us to combine and manipulate probabilities in a consistent and meaningful manner. You will describe ways of dealing with, and describing, uncertainty. This will involve rules which govern our use of terms like probability.\n\n\n\n\n\n\nNoteCommunicating uncertainty\n\n\n\nCommunicating probabilities, or uncertainty is one of the great challenges of scientists, economists and data analysts. Dealing in uncertainties is what we do. But often the people we are communicating to and with do not want to hear uncertainties, they want certainties. Will it rain tomorrow? Yes or No! Will my employer go out of business? Yes or no! Will the Conservatives be able to lead the government after the next election? Yes or no! Will this investment be profitable? Yes or no! Will the Apple share price go up? yes or no!\nDespite the thirst for certainty you will have to find ways to communicate uncertainty which both, conveys the right information (including the level of certainty you have), is understandable to the public, and protects your reputation. It turns out that this is a great challenge.\nHere is a great document on communicating uncertainty by Full Fact and a lecture on this issue (60 min) by the excellent David Spiegelhalter and a website he initiated.\nThis is supplementary and non-assessed information.\n\n\nThere have been a number of different approaches (interpretations) of probability. Most depend, at least to some extent, on the notion of relative frequency as now described:\n\nSuppose an experiment has an outcome of interest \\(E\\). The relative frequency interpretation of probability says that assuming the experiment can be repeated a large number of times then the relative frequency of observing the outcome \\(E\\) will settle down to a number, denoted \\(\\Pr (E)\\), \\(P(E)\\) or Prob\\((E)\\), called the probability of \\(E\\).\n\nThis is illustrated in the next Figure where the proportion of heads obtained after \\(n\\) flips of a fair coin is plotted against \\(n\\), as \\(n\\) increases; e.g., of the first 100 flips, 46 were heads (\\(46\\%\\)). Notice that the plot becomes less `wobbly’ after about \\(n=140\\) and appears to be settling down to the value of \\(\\frac{1}{2}\\).\n\n\n\nRelative frequency of heads in \\(n\\) toin tosses.\n\n\nDue to this interpretation of probability, we often use observed sample proportions to approximate underlying probabilities of interest. There are, of course, other interpretations of probability; e.g., the subjective interpretation which simply expresses the strength of one’s belief about an event of interest such as whether Liverpool will win the Premier League! Any one of these interpretations can be used in practical situations provided the implied notion of probability follows a simple set of axioms or rules.\n\n7.3.1 The axioms of probability\nThere are just three basic rules that must be obeyed when dealing with probabilities:\n\nFor any event \\(E\\) defined on \\(S\\) i.e., \\(E\\subset S,\\,\\,\\Pr (E)\\geq 0\\); probabilities are non-negative.\n\\(\\Pr (S)=1\\); having defined the sample space of outcomes, one of these outcomes must be observed.\nIf events \\(E\\) and \\(F\\) are mutually exclusive defined on \\(S\\), so that $EF=$, then \\(\\Pr \\left( E\\cup F\\right) =\\Pr \\left( E\\right)+\\Pr \\left( F\\right)\\). In general, for any set of mutually exclusive events, \\(E_{1},E_{2},\\ldots ,E_{k}\\), defined on \\(S:\\)\n\\[\\begin{equation*}\n      \\Pr (E_{1}\\cup E_{2}\\cup \\ldots \\cup E_{k})=\\Pr (E_{1})+\\Pr (E_{2})+\\ldots\\Pr (E_{k})\n  \\end{equation*}\\]\ni.e., \\(\\Pr \\left( \\bigcup_{j=1}^{k}E_{j}\\right) =\\sum_{j=1}^{k}\\Pr (E_{j})\\).\n\nIn terms of the Venn Diagram, one can (and should) usefully think of the area of \\(E\\) relative to that of \\(S\\) as providing an indication of probability. (Note, from axiom 2, that the area of \\(S\\) is implicitly normalised to be unity).\nAlso observe that, contrary to what you may have believed, it is not one of the rules that \\(\\Pr (E)\\leq 1\\) for any event \\(E\\). Rather, this is an implication of the three rules given:\n\n\n\n\n\n\nImportantImplications\n\n\n\nIt must be that for any event \\(E\\), defined on \\(S\\), $E{E}=$ and \\(E\\cup \\bar{E}=S\\). By Axiom 1, \\(\\Pr (E)\\geq 0\\) and \\(\\Pr \\left( \\bar{E}\\right) \\geq 0\\) and by Axiom 3, \\(\\Pr(E)+\\Pr (\\bar{E})=\\Pr (S)\\). So \\(\\Pr \\left( E\\right) +\\Pr \\left( \\bar{E}\\right) =1\\) by Axiom 2. This implies that\n\n\\(0\\leq \\Pr (E)\\leq 1\\)\n\\(\\Pr (\\bar{E})=1-\\Pr (E)\\)\n\nThe first of these is what we might have expected from probability (a number lying between \\(0\\) and \\(1\\)). The second implication is also very important; it says that the probability of \\(E\\) not happening is one minus the probability of it happening. Thus when rolling a die, the probability of getting \\(6\\) is one minus the probability of getting either a \\(1\\), \\(2\\), \\(3\\), \\(4\\) or \\(5\\).\n\n\nThese axioms imply how to calculate probabilities on a sample space of equally likely outcomes. For example, and as we have already noted, the experiment of rolling a fair dice defines a sample space of six, mutually exclusive and equally likely outcomes (1 to 6 dots on the up-turned face). The axioms then say that each of the six probabilities are positive, add to 1 and are all the same. Thus, the probability of any one of the outcomes must be simply \\(\\frac{1}{6}\\); which may accord with your intuition.\nA similar sort of analysis reveals that the probability of drawing a club from a deck of 52 cards is \\(\\frac{13}{52}\\) since any one of the 52 cards has an equal chance of being drawn and 13 of them are clubs; i.e., 13 of the 52 are clubs, so the probability of drawing a club is \\(\\frac{13}{52}\\). Notice the importance of the assumption of equally likely outcomes here.\nIn what follows you shall see how these axioms can be used. Firstly, consider the construction of a probability for the union of two events; i.e., the probability that either \\(E\\) or \\(F\\) or (perhaps) both will occur. Such a probability is embodied in the addition rule of probability.\n\n\n7.3.2 The addition rule of probability\nLet us begin with an example.\n\n7.3.2.1 Example\nWhen rolling a fair dice, let \\(E\\) denote the event of an “odd number of dots” and \\(F\\) the event of the “number of dots being greater than, or equal, to \\(4\\)”.\n\\(E = \\{1,3,5\\}\\), \\(Pr(E) = \\frac{3}{6}=0.5\\)\n\\(F = \\{4,5,6\\}\\), \\(Pr(F) = \\frac{3}{6}=0.5\\)\nWhat is the probability of the event \\(E\\cup F\\)? To calculate this we can collect together all the mutually exclusive (simple) events which comprise \\(E\\cup F\\), and then add up the probabilities (by axiom 3).\n\\(E\\cup F = \\{1,3,4,5,6 \\}\\), \\(Pr(E\\cup F) = \\frac{5}{6}\\)\n\\(E\\cap F = \\{5\\}\\), \\(Pr(E\\cap F) = \\frac{1}{6}\\)\nEach of the simple events has a probability of \\(\\frac{1}{6}\\) so the required total probability is: \\(\\Pr \\left( E\\cup F\\right) =\\frac{5}{6}\\). Consider carefully how this probability is constructed and note, in particular, that $( EF) ( E) +( F) $ since \\(E\\) and \\(F\\) have a simple event in common (namely 5 dots). As you will see shortly, \\(\\Pr \\left( E\\cup F\\right) = \\Pr \\left( E\\right) +\\Pr \\left( F\\right) - \\Pr(E \\cap F)\\).\nThis video goes through the above brief example (YouTube, 5min).\n{{ &lt; video https://www.youtube.com/watch?v=sFijzt3ByzY &gt; }}\n\nIn general, we can calculate the probability of the union of events using the addition rule of probability, as follows.\n\nFor any events, \\(E\\subset S\\) and \\(F\\subset S:\\Pr (E\\cup F)=\\Pr (E)+\\Pr (F)-\\Pr (E\\cap F)\\). So, in general, \\(\\Pr \\left( E\\cup F\\right) \\leq \\Pr (E)+\\Pr (F)\\).\n\nWe can demonstrate this as follows. Note that\n\\[\\begin{equation*}\nE\\cup F=\\left( E\\cap \\bar{F}\\right) \\cup \\left( E\\cap F\\right) \\cup \\left(\\bar{E}\\cap F\\right)\n\\end{equation*}\\]\nthe union of three mutually exclusive events. These mutually exclusive events are depicted by the shaded areas \\(\\mathbf{a}\\), \\(\\mathbf{b}\\) and \\(\\mathbf{c}\\), respectively, in the next Figure.\n\n\n\nVenn diagram to illustrate the addition rule.\n\n\nThen by Axiom 3, and from the fact that the three events $( E{F}) $, $( EF) $ and \\(\\left( \\bar{E}\\cap F\\right)\\) are mutually exclusive so that the “area” occupied by \\(E\\cup F\\) is simply \\(\\mathbf{a+b+c}\\),\n\\[\\begin{equation*}\n\\Pr \\left( E\\cup F\\right) =\\Pr \\left( E\\cap \\bar{F}\\right) +\\Pr \\left( \\bar{E}\\cap F\\right) +\\Pr \\left( E\\cap F\\right) .\n\\end{equation*}\\]\nBut also by Axiom 3, since $E=( E{F}) (EF) $, it must be that \\(\\Pr (E)=\\Pr \\left( E\\cap \\bar{F}\\right)+\\Pr (E\\cap F)\\); similarly, \\(\\Pr \\left( \\bar{E}\\cap F\\right) =\\Pr \\left(F\\right) -\\Pr \\left( E\\cap F\\right)\\). Putting all of this together gives\n\\[\\begin{equation*}\n\\Pr (E\\cup F)=\\Pr (E)+\\Pr (F)-\\Pr (E\\cap F).\n\\end{equation*}\\]\nThis was a lengthy probabilistic argument to make sense of the probability addition formula. You may wonder why the addition formula is not just \\(\\Pr (E\\cup F)=\\Pr (E)+\\Pr (F)\\)? You can refer back to the image above. If we just added \\(\\Pr (E)\\) and \\(\\Pr (F)\\) we would have double counted area b. We therefore have to subtract it once. And area b is \\(\\Pr (E\\cap F)\\).\nWhen \\(E\\) and \\(F\\) are mutually exclusive, then there is no overlap, so that \\(E\\cap F=\\emptyset\\), this rule reveals Axiom 2: \\(\\Pr (E\\cup F)=\\Pr (E)+\\Pr (F)\\).\n\n\n\n\n\n\nNoteExercise\n\n\n\nWhat is the probability of drawing a Queen (\\(Q\\)) or a Club (\\(C\\)) in a single draw from a pack of cards?\nThe probability is  (answer in decimals and rounded to 4dp)\n\n\nSolutions\n\n4 out of 52 cards are Queens, so \\(\\Pr \\left( Q\\right) =\\frac{4}{52}\\), whilst \\(\\Pr\\left( C\\right) =\\frac{13}{52}\\). The probability of drawing the Queen of Clubs is simply \\(\\frac{1}{52}\\); i.e., \\(\\Pr \\left( Q\\cap C\\right) =\\frac{1}{52}\\). What we require is a Club or a Queen, for which the probability is\n\\[\\begin{eqnarray*}\n\\Pr \\left( Q\\cup C\\right) &=&\\Pr \\left( Q\\right) +\\Pr \\left( C\\right) -\\Pr\\left( Q\\cap C\\right) \\\\\n&=&\\frac{4}{52}+\\frac{13}{52}-\\frac{1}{52} \\\\\n&=&\\frac{16}{52}=\\frac{4}{13} = 0.3077.\n\\end{eqnarray*}\\]\n\n\n\nYou will often be presented with frequency (count) tables of the following type.\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nVery good health\nGood health\nFair health\nBad health\nVery bad health\nSum\n\n\n\n\nNorth East\n11,425\n8,795\n3,977\n1,476\n428\n26,101\n\n\nNorth West\n32,413\n23,502\n9,847\n3,842\n1,083\n70,687\n\n\nLondon\n20,086\n18,302\n7,476\n2,376\n701\n52,943\n\n\nSum\n67,924\n50,599\n21,300\n7,694\n2,214\n149,731\n\n\n\nThese are the respondents from three UK regions (North East, North West and London) and their self indicated health status. The data are available in the 2011 Census Microdata Teaching File (see the Data Sets page). For instance there are 8,795 respondents from the North East that indicated that their health was best categorised as “Good health”. Throughout the three regions there were 50,599 respondents that gave “Good health” as their health status.\nNow you may be interested whether, for instance, respondents from the North West tend to be more positive about their health, compared to respondents from other regions. For this purpose it is useful to calculate a probability table based on the above frequency table. The way to do that is to divide all frequency values in the above table by the total number of responses, here 149,731. For example, for the above case of someone coming from the North East and indicating “Good health” we would calculate \\(8795/149731\\) which results in \\(0.0587\\). This is the probability that, if we randomly drew a person from our 149,731 responses, we would get someone from the North East with “Good health”, \\(Pr(\"North~East\" \\cap \"Good~health\")=0.0587\\). If you do that for all cells in the above table you get the following:\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nVery good health\nGood health\nFair health\nBad health\nVery bad health\nSum\n\n\n\n\nNorth East\n0.0763\n0.0587\n0.0266\n0.0099\n0.0029\n0.1743\n\n\nNorth West\n0.2165\n0.1570\n0.0658\n0.0257\n0.0072\n0.4721\n\n\nLondon\n0.1609\n0.1222\n0.0499\n0.0159\n0.0047\n0.3536\n\n\nSum\n0.4536\n0.3379\n0.1423\n0.0514\n0.0148\n1.000\n\n\n\nAs another example, the Pr(“London” “Verybadhealth”)=0.0047 implies that 0.47% of the sample are respondents from London with very bad health. We call these probabilities joint probabilities and in the above table they are coloured in blue. The probabilities at the margins of the table are also called marginal probabilities. For instance \\(Pr(\"London\")=0.3536\\), meaning that if we randomly draw a response from the above 149,731 respondents, there is a 35% probability that this response comes from London.\n\n7.3.2.2 Example\nConsider the following frequency table, where 9,900 respondents from two cities have been asked whether they enjoy eating vegetables.\n\n\n\n\n\n\n\n\n\n\nCity\nEnjoy (E)\nNeutral (N)\nDo not enjoy (D)\nSum\n\n\n\n\nSpringfield (S)\n\n1,045\n876\n2,673\n\n\nGotham City (G)\n2,893\n2,504\n1,803\n\n\n\nSum\n3,645\n\n2,706\n9,900\n\n\n\nComplete the above table:\nFrequency\\((\"Springfield\" \\cap \"Enjoy\")=\\) \nFrequency\\((\"Neutral\")=\\) \nFrequency\\((\"Gotham~City\")=\\) \nNow calculate the probability table (to 4dp:\n\n\n\n\n\n\n\n\n\n\nCity\nEnjoy (E)\nNeutral (N)\nDo not enjoy (D)\nSum\n\n\n\n\nSpringfield (S)\n\n\n\n\n\n\nGotham City (G)\n\n\n\n\n\n\nSum\n\n\n\n\n\n\n\nThe \\(Pr(\"Springfield\" \\cap \"Enjoy\")\\) is a marginaljoint probability.\nThe \\(Pr(\"Gotham~City\")\\) is a marginaljoint probability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducing Probability</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "8  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summary</span>"
    ]
  }
]