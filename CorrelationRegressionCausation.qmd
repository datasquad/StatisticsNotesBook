# Correlation, Regression and Causation

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(webexercises)
```


## Descriptive statistics for the relation between two or more variables

When we considered graphical ways to describe data we already considered the situation of having more than two variables. Very common graphical representations in such situations are line graphs (with multiple lines) when dealing with time series data or scatter plots when dealing with cross-sectional data. In particular scatter plots  allowed us to visualise how two series were related to each other.

Here we replicate a scatter plot used in an earlier section. This plot illustrates how the country level 14-day covid case rate (in 2022, Week 10) relates to the countries Health expenditure as a share of the countries GDP.

![Scatter diagram, Health expenditure as percentage of GDP and 14-day Covid case	rate, as per Week 10 in 2022.](images/scatter_covid_healthex.png)

It is somewhat difficult to see from this plot whether there was indeed a positive relationship between these two variables or not. In fact there are three aspects of a potential relationship you may be interested in. First, is it a positive or negative relationship, second, whether it appears to be a linear or nonlinear relationship and third, how strong is that relationship. In this section you will learn how to find numerical descriptive statistics that that speak to these aspects of such a relationship.

This video provides a short introduction to correlation

{{< https://youtu.be/cNyIMCjMvBA >}}

## Correlation

A commonly used measure of association is the **sample correlation coefficient**, which is designed to tell us something about the characteristics of a scatter plot of observations on the variable $Y$ against observations on the variable $X$. In particularly, are higher than average values of $Y$ associated with higher than average values of $X$, and vice-versa? In the context of the above example we would ask whether higher values of health expenditure (as \% of GDP) are related to higher Covid infection rates.

Consider the following data-set in which we observe the weight ($Y_i$) measured in pounds and the height ($X_i$) measured in inches of a sample of 12 people:

|                 | \emph{$i$} |     |     |     |     |     |     |     |     |     |     |     |
|-----------------|------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| \emph{Variable} | 1          | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  |
| Weight $=Y_i$   | 155        | 150 | 180 | 135 | 156 | 168 | 178 | 160 | 132 | 145 | 139 | 152 |
| Height $=X_i$   | 70         | 63  | 72  | 60  | 66  | 70  | 74  | 65  | 62  | 67  | 65  | 68  |

The best way to graphically represent the data is the following scatter plot:

![Scatter diagram, Weight and Height.](images/regression_scatter1.jpg)

On this graph a horizontal line at $y=154.1667$ (at the sample mean $\bar{y}$) and also a vertical line at $x=66.8333$ (at the sample mean $\bar{x}$) are superimposed. This creates four areas. Points in the upper right quadrant are those for which the weight is higher than average **and** height is higher than average. One of these points (Observation $i=3$ with $y_i=180, x_i=72$) is highlighted in the scatter plot along with its deviations from $\bar{x}$ and $\bar{y}$.

Points in the lower left quadrant are those for which weight is lower than average **and** height is lower than average. Since most points lie in these two quadrants, this suggests that higher than average weight is associated with higher than average height; whilst lower than average weight is associated with lower than average height. This is typical for a positive relationship between $X$ and $Y$. If there was no association, we would expect to see a roughly equal distribution of points in all four quadrants.

While it is often straightforward to see the qualitative nature of a relationship (positive, negative or unrelated) we want a numerical measure that describes this relationship such that we can also comment on the strength of the relationship. The basis of such a measure are again the deviations from the sample mean (as for the calculation of the variance and standard deviation), but now we have two such deviations for each observation, the deviation in the $Y$ variable, $d_{y,i}=(y_i-\bar{y})$, and the deviation in the $X$ variable, $d_{x,i}=(x_i-\bar{x})$. These are the deviations you can see in the above Figure.

In the case of the third observation with $y_3=180$ and $x_3=72$ we can see that both values are larger than the respective sample means $\bar{y}$ and $\bar{x}$ and therefore both, $d_{y,i}$ and $d_{x,i}$ are positive. \\
$d_{y,i}=(y_i-\bar{y})$ = 180-154.1667 = 25.8333\\
$d_{x,i}=(x_i-\bar{x})$ = 72-66.8333 = 5.1667

In fact this will be the case for all observations that lie in the **upper right** quadrant. For observations in the **lower left** quadrant we will find $d_{y,i}$ and $d_{x,i}$ to be smaller than 0. Observations in both these quadrants are reflective of a positive relationship. We therefore need to use the information in $d_{y,i}$ and $d_{x,i}$ in such a way that in both these cases we get a positive contribution to our statistic that numerically describes the relationship. Consider the term $(d_{y,i} \cdot d_{x,i})$; this term will be positive for all observations in either the **upper right** or **lower left** quadrant. For values in either the **upper left** or **lower right** quadrant, however, the terms $d_{y,i}$ and $d_{x,i}$ will have different signs and hence the term $(d_{y,i} \cdot d_{x,i})$ will be negative, reflective of the fact that observations in these quadrants are representative of a negative relationship.

It should now be no surprise to find that our numerical measure of a relationship between two variables is based on these terms. In particular we will use what is called the sample covariance:

\begin{equation*}
	Cov(X,Y)=s_{X,Y}=\frac{1}{n-1}\sum_{i=1}^{n}\left( x_{i}-\bar{x}\right) \left( y_{i}-\bar{y}\right) =\frac{1}{n-1}\sum_{i=1}^{n} d_{x,i} d_{y,i} 
\end{equation*}

You can see that this is the sum of  $d_{y,i}d_{x,i}$ divided by $n-1$. The reason for dividing by $n-1$ and not $n$ is similar to the reasoning used for the sample variance. In fact the measure that we typically use is the \emph{correlation coefficient}:

\begin{eqnarray*}
	Corr(X,Y)=r_{XY}&=&\frac{s_{X,Y} }{\sqrt{s^2_X s^2_Y}}\\
	&=& \frac{\frac{1}{n-1}\sum_{i=1}^{n}\left( x_{i}-\bar{x}\right) \left( y_{i}-\bar{y}\right) }{\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}\left( x_{i}-\bar{x}\right)^{2}\,\frac{1}{n-1}\sum_{i=1}^{n}\left( y_{i}-\bar{y}\right) ^{2}}}\\
	&=& \frac{\sum_{i=1}^{n}\left( x_{i}-\bar{x}\right) \left( y_{i}-\bar{y}\right) }{\sqrt{\sum_{i=1}^{n}\left( x_{i}-\bar{x}\right)^{2}\,\sum_{i=1}^{n}\left( y_{i}-\bar{y}\right) ^{2}}}\\
\end{eqnarray*}

It is the sample covariance divided by the square root of the product of the two sample variances. In the last line we merely cancelled out the $1/(n-1)$ terms.

::: callout-info

#### Excel application

The calculations are best done in a Table format or using Excel. 

This video shows the calculations by hand (YouTube, 21min)

{{< https://www.youtube.com/watch?v=xMDjtzT1G_w >}}

This video shows the calculations by Excel (YouTube, 11min)

{{< https://www.youtube.com/watch?v=YdtXe2vkKrw >}}

| Obs | Weight (Y) | Height (X) | \(y_i-\bar{y}$ | \(x_i-\bar{x}$ | \((y_i-\bar{y})^2$ | \((x_i-\bar{x})^2$ | \((y_i-\bar{y})(x_i-\bar{x})$ |
|-----|------------|------------|-----------------|-----------------|---------------------|---------------------|--------------------------------|
| 1   | 155        | 70         | 0.8333          | 3.1667          | 0.6944              | 10.0278             | 2.6389                         |
| 2   | 150        | 63         | -4.1667         | -3.8333         | 17.3611             | 14.6944             | 15.9722                        |
| 3   | 180        | 72         | 25.8333         | 5.1667          | 667.3611            | 26.6944             | 133.4722                       |
| 4   | 135        | 60         | -19.1667        | -6.8333         | 367.3611            | 46.6944             | 130.9722                       |
| 5   | 156        | 66         | 1.8333          | -0.8333         | 3.3611              | 0.6944              | -1.5278                        |
| 6   | 168        | 70         | 13.8333         | 3.1667          | 191.3611            | 10.0278             | 43.8056                        |
| 7   | 178        | 74         | 23.8333         | 7.1667          | 568.0278            | 51.3611             | 170.8056                       |
| 8   | 160        | 65         | 5.8333          | -1.8333         | 34.0278             | 3.3611              | -10.6944                       |
| 9   | 132        | 62         | -22.1667        | -4.8333         | 491.3611            | 23.3611             | 107.1389                       |
| 10  | 145        | 67         | -9.1667         | 0.1667          | 84.0278             | 0.0278              | -1.5278                        |
| 11  | 139        | 65         | -15.1667        | -1.8333         | 230.0278            | 3.3611              | 27.8056                        |
| 12  | 152        | 68         | -2.1667         | 1.1667          | 4.6944              | 1.3611              | -2.5278                        |
| Sum | 1850       | 802        | 0               | 0               | 2659.6667           | 191.6667            | 616.3333                       |

: Table illustrating the calculations required to calculate a covariance. Values rounded to 4dp.


If you calculate $r$ for the above example you should obtain a value of 

\begin{equation*}
	Corr(X,Y)=r_{XY}=\frac{616.3333}{\sqrt{2659.6667 \cdot 191.6667}} = 0.8632
\end{equation*}

:::


::: callout-important

#### Exercise

Complete the table below and calculate, sample means, sample variances and standard deviations, the sample covariance and the correlation for the following four observations of $Y$ and $X$. 

| Obs | Weight (Y) | Height (X) | $y_i-\bar{y}$   | $x_i-\bar{x}$   | $(y_i-\bar{y})^2$   | $(x_i-\bar{x})^2$   | $(y_i-\bar{y})(x_i-\bar{x})$   |
|-----|------------|------------|-----------------|-----------------|---------------------|---------------------|--------------------------------|
| 1   | 4          | 10         | -4              | 6               | 16                  | 36                  | -24                            |
| 2   | 8          | -2         | 0               | -6              | 0                   | 36                  | 0                              |
| 3   | 6          | 7          |                 |                 |                     |                     |                                |
| 4   | 14         | 1          |                 |                 |                     |                     |                                |
|-----|------------|------------|-----------------|-----------------|---------------------|---------------------|--------------------------------|
| Sum | 32         | 16         |                 |                 |                     |                     |                                |

: Table for correlation calculations.

$\bar{y} =$ `r fitb(8)`

$\bar{x} =$ `r fitb(4)`

$s_Y^2 =$ `r fitb(18.6667)`

$s_X^2 =$ `r fitb(30)`

$Cov(X,Y) = s_{Y,X} =$ `r fitb(-16)`

$Corr(X,Y) = r =$ `r fitb(-0.6761)`

:::

A few things are worth noting with respect to the **correlation coefficient**:

* It can be shown algebraically that $-1 \leq r \leq 1$.
* Positive (negative) numbers represent a positive (negative) relationship and a value of 0 represents the absence of any relationship. In our Excel application example $r=0.8632$ and hence the two variables display a strong positive correlation.

![Correlation coefficient, strength and direction.](images/correlation_coefficient.png)


* The numerator contains the sum of the discussed cross products $d_{y,i} \cdot d_{x,i}=(y_i-\bar{y})(x_i-\bar{x})$
* The term in the denominator of the equation for $r$ is related to the variances of $Y$ and $X$. These terms are required to **standardise** the statistic to be between -1 and 1.
* For the correlation the order of variables does not matter, i.e. $r_{XY}=r_{YX}$.

The **covariance** is actually also a measure of the relationship between these two variables, but it has many of the same shortcomings as the variance (see the Descriptive Statistics lesson). Therefore we want a standardised measure (to ensure that $-1\leq r \geq 1$. This standardisation uses the square root of the two respective variances.

There are two very important limitations of the **correlation coefficient**:

* In general, this sort of analysis does not imply causation, in either direction. Variables may appear to move together for a number of reasons and not because one is causally linked to the other. 

::: callout-caution

#### Correlation, NOT Causation

For example, over the period 1945-64 the number of TV licences $(x)$ taken out in the UK increased steadily, as did the number of convictions for juvenile delinquency $\left( y \right)$. Thus a scatter of $y$ against $x$, and the construction of the sample correlation coefficient reveals an apparent positive relationship. However, to therefore claim that increased exposure to TV causes juvenile delinquency would be extremely irresponsible.

:::
		
Another example illustrating that correlation must not be mis-interpreted as a causal relationship is that of ice cream sales and shark attacks, illustrated in the following image.

![Correlation is not causation.](images/causation_correlation.png)

If we were to calculate a correlation between the two series we clearly would obtain a positive correlation. But nobody would suggest that increased ice-cream sales cause shark attacks, or indeed that increased shark attacks caused higher ice cream sales.  
Here is a different discussion on this issue:Â Khan Academy: [Do not confuse correlation with causation](www.khanacademy.org/math/probability/regression/regression-correlation/v/correlation-and-causality).

* The sample correlation coefficient gives an index of the apparent linear relationship only. It **assumes** that the scatter of points must be distributed about some underlying straight line. This is discussed further below. However, the term relationship is not really confined to such linear relationships. 

::: callout-caution

#### Relationship, NOT Linear Relationship

Consider the relationship between \emph{age} and \emph{income}. If we were to plot observations for the age and income of people in the age range of 20 to 50 we will clearly find a positive relationship. However, if we were to extend the age range to 80, we would most likely see that income decreases at the upper end of the age range. Therefore there is no linear age/income relationship across the full age range and the **correlation coefficient** cannot be used to describe such a relationship.

:::
	
Imagine drawing a straight line of **best fit** through the scatter of points in the above Figure (for height and weight) simply from **visual** inspection. You would try and make it **go through** the scatter, in some way, and it would probably have a positive slope. Numerically, one of the things that the correlation coefficient does is assess the slope of such a line: if $r>0$, then the slope of the line of best fit should be positive, and vice-versa. Moreover, if $r$ is close to either 1 (or -1) then this implies that the scatter is quite closely distributed around the line of best fit. What the correlation coefficient doesn't do, however, is tell us the exact position of line of best fit. This is achieved using **regression** analysis.

![Scatter plots and correlation.](images/Scatterplot_correlation.jpeg)

::: callout-important

#### Exercise

Consider the following Figure which displays six scatterplots 

![Scatter plots of six bivariate data samples.](images/SixCorrelations.png)

Match the correlations to the plots with the correlations. Note that the scales are identical for all plots.:

$r_1 = 0.9226$: Plot `r fitb(1)`
$r_2 = -0.8835$: Plot `r fitb(2)`
$r_3 = 0.9931$ Plot `r fitb(3)`
$r_4 = -0.9936$ Plot `r fitb(4)`
$r_5 = -0.1988$ Plot `r fitb(5)`
$r_6 = 0.4813$ Plot `r fitb(6)`

`r hide("Feedback")`

The sign of the correlation tells you whether the correlation is positive or negative. The strength of the correlation tells you how close the points are to a straight line (not shown here but in the next section).

It is not straightforward to tell the plots for the last two correlations apart. Plot 6 shows some more points in the top right quadrant and hence is the one with the positive correlation.

`r unhide()`

:::

### Additional resources

* [Khan Academy: Do not confuse correlation with causation](https://www.khanacademy.org/math/probability/regression/regression-correlation/v/correlation-and-causality)

## Regression

When thinking about correlations you learned that correlation values close to 1 or -1 imply that the points will lie close to an imaginary line,  the "line of best fit". The following image shows the six scatter plots we looked at above but now including the lines of best fit.

![Scatter plots of six bivariate data samples and lines of best fit.](images/SixCorrelations_wLOBF.png)

The lines of best fit drawn on the scatter can be represented algebraically as $a+bx$. Here $x$ represents the value on the horizontal axis, $a$ is the intercept (i.e. the value on the vertical axis at $x=0$) and $b$ is the slope (i.e. the value by which the line increases as we increase $x$ by one unit). The line is defined at any value of $x$ and not only those at which we have actual observations. Here you can see the line of best fit for for sample 1 with an indication of the intercept ($a$) and the slope ($b$). For the data in Sample 1 these values are $a=-0.34$ and $b=1.1327$. Just take these as given for now, you will shortly learn how to calculate these.

![Scatter plot for Sample 1 and line of best fit (red).](images/Scatter_LOBF_Details.png)

When you substitute any of the observed values $x_i$ into the line of best fit you get $\widehat{y}_i=a+bx_i$. It is important to note that the result of this operation, $\widehat{y}_i$ is not the same as $y_i$. Let us illustrate this in the above scatter plot. One of the scatter points is for $x_i=6$ and $y_i=9$ (the point is highlighted in red in the next plot.) If we plug in the values for $a$ and $b$, we get

\begin{equation*}
	\widehat{y}_i=a+bx_i = -0.34 + 1.1327 \cdot 6 = 6.4562 
\end{equation*}

The difference between the two is what is often called the residual:

\begin{equation*}
	res_i=y_i-\widehat{y}_i =  y_i - (a + bx_i) = y_i - a - bx_i.
\end{equation*}

Here that residual is $res_i=y_i-\widehat{y}_i =9-6.4562 = 2.538$.

![Scatter plot for Sample 1, line of best fit and example residual.](images/Scatter_LOBF_Details2.png)
: Scatter plot for Sample 1, line of best fit and example residual.




















